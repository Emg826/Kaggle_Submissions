# Machine Learning - Kaggle Submissions
Kaggle.com is a website that hosts machine learning data sets and many machine learning competitions. This repository is a collection of python notebooks detailing my submissions to various contests on the website. By "detailing" I mean that I try to walkthrough the entire process of model building: data cleaning, feature engineering, imputing missing values, and, of course, making predictions. 

Files
+ titanic_survivor_predictions.ipynb: try to predict who did and did not survive the Titanic. This required feature engineering, missing value imputation, and testing various algorithms for generalizablity. Achieved 79.4% accuracy, which, as of Jun. 18, 2018, puts the model at **2,532nd place out of 11,356 people**. 

+ concrete_strength_prediction.ipynb: try to predict the concrete compressive strength, which is how much pressure concrete can withstand. Compressive strength is measure in mega Pascals (MPa), and 1 MPa is about 145 psi according to Google. Trying to predict a continuous value meant that I used mean squared error (MSE) to evaluate different models. The baseline model, linear regression, had an MSE of about 120 while the best model, which split the data into 2 subsets and trained 2 models on those 2 subsets, had an MSE of about 23. This means that the average guesses were off by 11 MPa and 4.8 MPa, respectively. This was not a competition, so there is no leaderboard placement to report.

+ house_sale_price_predicting.ipynb: try to predict the sale price of houses in Ames, Iowa. A lot of feature engineering was required to due to the dimensionality of the data set: there were 79 variables. This involved binary, ordinal, and dummy encoding columns, replacing and imputing missing values, and adding some columns of my own by synthesizing information from other columns. Between the support vector regressor, the lasso regressor, linear regression, and the random forest, the best model was the random forest. The random forest's predictions were submitted to Kaggle and scored a root mean squared log error (the metric for the competition) of 0.14447. This placed the model at **2,272nd place out of 4,969 teams**.
