{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Rare Decay Phenomenon: τ → μμμ\n",
    "This notebook tries to identify a supposed rare decay phenomenon of a subatomic particle: τ → μμμ. This decay has not or cannot be found in nature because it violates the law of conservation of energy. Nevertheless, in theory, it can exist. Therefore, the objective of this notebook is to maximize a weighted area under the receiver operating characterisitc curve (AUC or AUROC). \n",
    "\n",
    "AUROC has the true-positive rate (correctly classified as hit/decay) as some function of the false-positive rate (incorrectly classified as hit/decay): true-positive-rate = f(false-positive-rate). The best value is 1.0, and the worst value is 0.0. AUROC is used when one needs to be very sure that if a model says 'hit' that the instance is actually a hit.\n",
    "\n",
    "In addition to maximizing the weighted AUROC, there are two modeling conditions: passing an agreement test and passing a correlation test. Because this phenomenon has not or cannot be found in nature, a Monte Carlo simulation (based on theory) generated data for instances of the rare decay phenomenon. These two tests are meant to ensure that a machine learning algorithm is not using the imperfections in the Monte Carlo simulation in its classification decisions.\n",
    "\n",
    "This notebook is for a Kaggle competition, Flavours of Physics: Finding τ → μμμ (Kernels Only), which can be found here: https://www.kaggle.com/c/flavours-of-physics-kernels-only. Other functions that are drawn on the competition administrators' listed resources should have links attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy \n",
    "\n",
    "\n",
    "# Kaggle csv imports\n",
    "# train = pandas.read_csv('../input/training.csv', index_col='id') # data on which to train models\n",
    "# test = pandas.read_csv('../input/test.csv', index_col='id') # data on which final predictions will be made\n",
    "# check_correlation = pandas.read_csv('../input/check_correlation.csv', index_col='id') # data on which to do correlation test\n",
    "# check_agreement = pandas.read_csv('../input/check_agreement.csv', index_col='id') # data on which to do the agreement test\n",
    "\n",
    "# My machine csv imports\n",
    "train = pandas.read_csv('training.csv', index_col='id') # data on which to train models\n",
    "test = pandas.read_csv('test.csv', index_col='id') # data on which final predictions will be made\n",
    "check_correlation = pandas.read_csv('check_correlation.csv', index_col='id') # data on which to do correlation test\n",
    "check_agreement = pandas.read_csv('check_agreement.csv', index_col='id') # data on which to do the agreement test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/yandexdataschool/flavours-of-physics-start/blob/master/evaluation.py\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "def __rolling_window(data, window_size):\n",
    "    \"\"\"\n",
    "    Rolling window: take window with definite size through the array\n",
    "    :param data: array-like\n",
    "    :param window_size: size\n",
    "    :return: the sequence of windows\n",
    "    Example: data = array(1, 2, 3, 4, 5, 6), window_size = 4\n",
    "        Then this function return array(array(1, 2, 3, 4), array(2, 3, 4, 5), array(3, 4, 5, 6))\n",
    "    \"\"\"\n",
    "    shape = data.shape[:-1] + (data.shape[-1] - window_size + 1, window_size)\n",
    "    strides = data.strides + (data.strides[-1],)\n",
    "    return numpy.lib.stride_tricks.as_strided(data, shape=shape, strides=strides)\n",
    "\n",
    "\n",
    "def __cvm(subindices, total_events):\n",
    "    \"\"\"\n",
    "    Compute Cramer-von Mises metric.\n",
    "    Compared two distributions, where first is subset of second one.\n",
    "    Assuming that second is ordered by ascending\n",
    "    :param subindices: indices of events which will be associated with the first distribution\n",
    "    :param total_events: count of events in the second distribution\n",
    "    :return: cvm metric\n",
    "    \"\"\"\n",
    "    target_distribution = numpy.arange(1, total_events + 1, dtype='float') / total_events\n",
    "    subarray_distribution = numpy.cumsum(numpy.bincount(subindices, minlength=total_events), dtype='float')\n",
    "    subarray_distribution /= 1.0 * subarray_distribution[-1]\n",
    "    return numpy.mean((target_distribution - subarray_distribution) ** 2)\n",
    "\n",
    "\n",
    "def compute_cvm(predictions, masses, n_neighbours=200, step=50):\n",
    "    \"\"\"\n",
    "    Computing Cramer-von Mises (cvm) metric on background events: take average of cvms calculated for each mass bin.\n",
    "    In each mass bin global prediction's cdf is compared to prediction's cdf in mass bin.\n",
    "    :param predictions: array-like, predictions\n",
    "    :param masses: array-like, in case of Kaggle tau23mu this is reconstructed mass\n",
    "    :param n_neighbours: count of neighbours for event to define mass bin\n",
    "    :param step: step through sorted mass-array to define next center of bin\n",
    "    :return: average cvm value\n",
    "    \"\"\"\n",
    "    predictions = numpy.array(predictions)\n",
    "    masses = numpy.array(masses)\n",
    "    assert len(predictions) == len(masses)\n",
    "\n",
    "    # First, reorder by masses\n",
    "    predictions = predictions[numpy.argsort(masses)]\n",
    "\n",
    "    # Second, replace probabilities with order of probability among other events\n",
    "    predictions = numpy.argsort(numpy.argsort(predictions, kind='mergesort'), kind='mergesort')\n",
    "\n",
    "    # Now, each window forms a group, and we can compute contribution of each group to CvM\n",
    "    cvms = []\n",
    "    for window in __rolling_window(predictions, window_size=n_neighbours)[::step]:\n",
    "        cvms.append(__cvm(subindices=window, total_events=len(predictions)))\n",
    "    return numpy.mean(cvms)\n",
    "\n",
    "\n",
    "def __roc_curve_splitted(data_zero, data_one, sample_weights_zero, sample_weights_one):\n",
    "    \"\"\"\n",
    "    Compute roc curve\n",
    "    :param data_zero: 0-labeled data\n",
    "    :param data_one:  1-labeled data\n",
    "    :param sample_weights_zero: weights for 0-labeled data\n",
    "    :param sample_weights_one:  weights for 1-labeled data\n",
    "    :return: roc curve\n",
    "    \"\"\"\n",
    "    labels = [0] * len(data_zero) + [1] * len(data_one)\n",
    "    weights = numpy.concatenate([sample_weights_zero, sample_weights_one])\n",
    "    data_all = numpy.concatenate([data_zero, data_one])\n",
    "    fpr, tpr, _ = roc_curve(labels, data_all, sample_weight=weights)\n",
    "    return fpr, tpr\n",
    "\n",
    "\n",
    "def compute_ks(data_prediction, mc_prediction, weights_data, weights_mc):\n",
    "    \"\"\"\n",
    "    Compute Kolmogorov-Smirnov (ks) distance between real data predictions cdf and Monte Carlo one.\n",
    "    :param data_prediction: array-like, real data predictions\n",
    "    :param mc_prediction: array-like, Monte Carlo data predictions\n",
    "    :param weights_data: array-like, real data weights\n",
    "    :param weights_mc: array-like, Monte Carlo weights\n",
    "    :return: ks value\n",
    "    \"\"\"\n",
    "    assert len(data_prediction) == len(weights_data), 'Data length and weight one must be the same'\n",
    "    assert len(mc_prediction) == len(weights_mc), 'Data length and weight one must be the same'\n",
    "\n",
    "    data_prediction, mc_prediction = numpy.array(data_prediction), numpy.array(mc_prediction)\n",
    "    weights_data, weights_mc = numpy.array(weights_data), numpy.array(weights_mc)\n",
    "\n",
    "    assert numpy.all(data_prediction >= 0.) and numpy.all(data_prediction <= 1.), 'Data predictions are out of range [0, 1]'\n",
    "    assert numpy.all(mc_prediction >= 0.) and numpy.all(mc_prediction <= 1.), 'MC predictions are out of range [0, 1]'\n",
    "\n",
    "    weights_data /= numpy.sum(weights_data)\n",
    "    weights_mc /= numpy.sum(weights_mc)\n",
    "\n",
    "    fpr, tpr = __roc_curve_splitted(data_prediction, mc_prediction, weights_data, weights_mc)\n",
    "\n",
    "    Dnm = numpy.max(numpy.abs(fpr - tpr))\n",
    "    return Dnm\n",
    "\n",
    "\n",
    "def roc_auc_truncated(labels, predictions, tpr_thresholds=(0.2, 0.4, 0.6, 0.8),\n",
    "                      roc_weights=(4, 3, 2, 1, 0)):\n",
    "    \"\"\"\n",
    "    Compute weighted area under ROC curve.\n",
    "    :param labels: array-like, true labels\n",
    "    :param predictions: array-like, predictions\n",
    "    :param tpr_thresholds: array-like, true positive rate thresholds delimiting the ROC segments\n",
    "    :param roc_weights: array-like, weights for true positive rate segments\n",
    "    :return: weighted AUC\n",
    "    \"\"\"\n",
    "    assert numpy.all(predictions >= 0.) and numpy.all(predictions <= 1.), 'Data predictions are out of range [0, 1]'\n",
    "    assert len(tpr_thresholds) + 1 == len(roc_weights), 'Incompatible lengths of thresholds and weights'\n",
    "    fpr, tpr, _ = roc_curve(labels, predictions)\n",
    "    area = 0.\n",
    "    tpr_thresholds = [0.] + list(tpr_thresholds) + [1.]\n",
    "    for index in range(1, len(tpr_thresholds)):\n",
    "        tpr_cut = numpy.minimum(tpr, tpr_thresholds[index])\n",
    "        tpr_previous = numpy.minimum(tpr, tpr_thresholds[index - 1])\n",
    "        area += roc_weights[index - 1] * (auc(fpr, tpr_cut, reorder=True) - auc(fpr, tpr_previous, reorder=True))\n",
    "    tpr_thresholds = numpy.array(tpr_thresholds)\n",
    "    # roc auc normalization to be 1 for an ideal classifier\n",
    "    area /= numpy.sum((tpr_thresholds[1:] - tpr_thresholds[:-1]) * numpy.array(roc_weights))\n",
    "    return area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the evaluation functions provided by the administrators of the \n",
    "# competition (found here: \n",
    "# https://github.com/yandexdataschool/flavours-of-physics-start/blob/master/baseline.ipynb)\n",
    "def evaluate_model(model, variables, validation, check_agreement, check_correlation):\n",
    "    \"\"\"\n",
    "    This function combines the evaluation of the agreement test (compute_ks()), the correlation\n",
    "    test (compute_cvm()), and the area under the curve score (roc_auc_truncated()) for\n",
    "    a given model.\n",
    "    \n",
    "    Assumptions: \n",
    "    i. all of the functions utilized herein have already been created\n",
    "    ii. model has a method called predict_proba() (i.e., model is a classifier)\n",
    "    iii. if the model requires any kind of scaling or has any other preprocessing,\n",
    "    'model' should be a pipeline object that includes that preprocessing\n",
    "    \n",
    "    :param model: the already-fitted classification model or pipeline with the model\n",
    "    :param variables: list of variables that the model uses in the same order \n",
    "        that the model expects them -- SHOULD NOT include target variable\n",
    "    :param train: pandas dataframe of train.csv\n",
    "    :param check_agreement: pandas dataframe of check_agreement.csv\n",
    "    :param check_correlation: pandas dataframe of check_correlation.csv\n",
    "    \n",
    "    :returns: AUC if ALL tests passed; -1.0 * AUC otherwise\n",
    "    \"\"\"\n",
    "    print('TEST RESULTS')\n",
    "    \n",
    "    # Agreement test\n",
    "    agreement_probs = model.predict_proba(check_agreement[variables])[:, 1]\n",
    "\n",
    "    ks = compute_ks(agreement_probs[check_agreement['signal'].values == 0],\n",
    "                    agreement_probs[check_agreement['signal'].values == 1],\n",
    "                    check_agreement[check_agreement['signal'] == 0]['weight'].values,\n",
    "                    check_agreement[check_agreement['signal'] == 1]['weight'].values)\n",
    "\n",
    "    ks_test_passed = ks < 0.09\n",
    "    print ('KS metric: \\t {0:.4f} < {1}? \\t\\t{2}.'.format(ks, 0.09, ks_test_passed)) # 0.09 specified by admin\n",
    "    \n",
    "    # Correlation test\n",
    "    correlation_probs = model.predict_proba(check_correlation[variables])[:, 1]\n",
    "    cvm = compute_cvm(correlation_probs, check_correlation['mass'])\n",
    "    \n",
    "    cv_test_passed = cvm < 0.002\n",
    "    print('CvM metric: \\t {0:.5f} < {1}? \\t\\t{2}.'.format(cvm, 0.002, cv_test_passed)) # 0.002 specified by admin\n",
    "    \n",
    "    # Weighted AUC on validation data\n",
    "    validation_probs = model.predict_proba(validation[variables])[:, 1]\n",
    "    auc = roc_auc_truncated(validation['signal'], validation_probs)\n",
    "    \n",
    "    auc_test_passed = auc > 0.834346386627 # 0.83-etc. comes from admin's baseline.ipynb \n",
    "    print('AUC: \\t\\t {0:.7f} > {1:.7f}? \\t{2}.'.format(auc, 0.834346386627, auc_test_passed))\n",
    "    # 0.834... comes from baseline.ipynb on admin github page\n",
    "    \n",
    "    # This is a bit like returning 2 things: + ==> all tests passed while\n",
    "    # - ==> not all tests passed AND numpy.abs(AUC) is the model's AUC score\n",
    "    # on validation data\n",
    "    if auc_test_passed and cv_test_passed and ks_test_passed:\n",
    "        return auc\n",
    "    else: \n",
    "        return -1.0 * auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often times for classification problems, a good place to start is logistic regression. Therefore, the first model will just be a logistic regression using all of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns that check_agreement.csv, check_correlation.csv, train.csv, and test.csv all have,\n",
    "# minus the target variable: signal\n",
    "unusable_columns = ['production', 'mass', 'min_ANNmuon', 'signal']\n",
    "ubiquitous_columns = [col for col in train.columns.tolist() if col not in unusable_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST RESULTS\n",
      "KS metric: \t 0.1601 < 0.09? \t\tFalse.\n",
      "CvM metric: \t 0.00081 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.9320031 > 0.8343464? \tTrue.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.9320031311142262"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# A train/test split, sort of\n",
    "validation = train.sample(frac=0.75, random_state=3) # train=25%; validation=75% -- reversed for training speed\n",
    "train = train.drop(validation.index)\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "validation = validation.reset_index(drop=True)\n",
    "\n",
    "lrcv = LogisticRegressionCV(random_state=3)\n",
    "lrcv.fit(train[ubiquitous_columns], train['signal'])\n",
    "\n",
    "evaluate_model(lrcv, ubiquitous_columns, validation, check_agreement, check_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that didn't work. Let's just try each a logistic regression with each variable on its own. For each variable that passes all 3 tests, add it to a list so that the next logistic regressor can be trained on all of those columns whose individual logistic regressions passed all 3 tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LifeTime\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0823 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00100 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.7680600 > 0.8343464? \tFalse.\n",
      "\n",
      "dira\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0344 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00128 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.9339494 > 0.8343464? \tTrue.\n",
      "\n",
      "FlightDistance\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.1074 < 0.09? \t\tFalse.\n",
      "CvM metric: \t 0.00098 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.7498143 > 0.8343464? \tFalse.\n",
      "\n",
      "FlightDistanceError\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0533 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00091 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.7416115 > 0.8343464? \tFalse.\n",
      "\n",
      "IP\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0735 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00108 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.9505927 > 0.8343464? \tTrue.\n",
      "\n",
      "IPSig\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0695 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00096 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.9530628 > 0.8343464? \tTrue.\n",
      "\n",
      "VertexChi2\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0231 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00087 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.8429456 > 0.8343464? \tTrue.\n",
      "\n",
      "pt\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0583 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00098 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.7387655 > 0.8343464? \tFalse.\n",
      "\n",
      "DOCAone\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0254 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00088 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.7775152 > 0.8343464? \tFalse.\n",
      "\n",
      "DOCAtwo\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0226 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00085 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.7787682 > 0.8343464? \tFalse.\n",
      "\n",
      "DOCAthree\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0270 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00095 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.7686757 > 0.8343464? \tFalse.\n",
      "\n",
      "IP_p0p2\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0558 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00073 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.7372416 > 0.8343464? \tFalse.\n",
      "\n",
      "IP_p1p2\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0954 < 0.09? \t\tFalse.\n",
      "CvM metric: \t 0.00090 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.7961173 > 0.8343464? \tFalse.\n",
      "\n",
      "isolationa\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0184 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00418 < 0.002? \t\tFalse.\n",
      "AUC: \t\t 0.8294140 > 0.8343464? \tFalse.\n",
      "\n",
      "isolationb\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0351 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00314 < 0.002? \t\tFalse.\n",
      "AUC: \t\t 0.7992357 > 0.8343464? \tFalse.\n",
      "\n",
      "isolationc\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0431 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00407 < 0.002? \t\tFalse.\n",
      "AUC: \t\t 0.7936578 > 0.8343464? \tFalse.\n",
      "\n",
      "isolationd\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0097 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.03695 < 0.002? \t\tFalse.\n",
      "AUC: \t\t 0.7080520 > 0.8343464? \tFalse.\n",
      "\n",
      "isolatione\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0109 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.03782 < 0.002? \t\tFalse.\n",
      "AUC: \t\t 0.7083355 > 0.8343464? \tFalse.\n",
      "\n",
      "isolationf\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0043 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.03853 < 0.002? \t\tFalse.\n",
      "AUC: \t\t 0.7049590 > 0.8343464? \tFalse.\n",
      "\n",
      "iso\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0193 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.01359 < 0.002? \t\tFalse.\n",
      "AUC: \t\t 0.7977217 > 0.8343464? \tFalse.\n",
      "\n",
      "CDF1\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0501 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00216 < 0.002? \t\tFalse.\n",
      "AUC: \t\t 0.7421668 > 0.8343464? \tFalse.\n",
      "\n",
      "CDF2\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0417 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00103 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.7995085 > 0.8343464? \tFalse.\n",
      "\n",
      "CDF3\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0225 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00081 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.7999493 > 0.8343464? \tFalse.\n",
      "\n",
      "ISO_SumBDT\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0463 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00086 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.8948137 > 0.8343464? \tTrue.\n",
      "\n",
      "p0_IsoBDT\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0273 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00086 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.8858454 > 0.8343464? \tTrue.\n",
      "\n",
      "p1_IsoBDT\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0394 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00089 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.8802729 > 0.8343464? \tTrue.\n",
      "\n",
      "p2_IsoBDT\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0363 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00100 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.8753440 > 0.8343464? \tTrue.\n",
      "\n",
      "p0_track_Chi2Dof\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0763 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00080 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.7993241 > 0.8343464? \tFalse.\n",
      "\n",
      "p1_track_Chi2Dof\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0567 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00078 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.7329789 > 0.8343464? \tFalse.\n",
      "\n",
      "p2_track_Chi2Dof\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0633 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00075 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.7189893 > 0.8343464? \tFalse.\n",
      "\n",
      "p0_IP\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0222 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00093 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.7818972 > 0.8343464? \tFalse.\n",
      "\n",
      "p1_IP\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0852 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00116 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.7087765 > 0.8343464? \tFalse.\n",
      "\n",
      "p2_IP\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0915 < 0.09? \t\tFalse.\n",
      "CvM metric: \t 0.00070 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.7176464 > 0.8343464? \tFalse.\n",
      "\n",
      "p0_IPSig\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0305 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00061 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.8126635 > 0.8343464? \tFalse.\n",
      "\n",
      "p1_IPSig\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0753 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00099 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.7429791 > 0.8343464? \tFalse.\n",
      "\n",
      "p2_IPSig\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0735 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00081 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.7019376 > 0.8343464? \tFalse.\n",
      "\n",
      "p0_pt\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0688 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00090 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.7408151 > 0.8343464? \tFalse.\n",
      "\n",
      "p1_pt\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0341 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00071 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.7483952 > 0.8343464? \tFalse.\n",
      "\n",
      "p2_pt\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0408 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00076 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.6716655 > 0.8343464? \tFalse.\n",
      "\n",
      "p0_p\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0826 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00063 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.7163864 > 0.8343464? \tFalse.\n",
      "\n",
      "p1_p\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0369 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00089 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.7155337 > 0.8343464? \tFalse.\n",
      "\n",
      "p2_p\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0270 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00074 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.7025298 > 0.8343464? \tFalse.\n",
      "\n",
      "p0_eta\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0421 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00093 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.7416590 > 0.8343464? \tFalse.\n",
      "\n",
      "p1_eta\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0375 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00091 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.7386283 > 0.8343464? \tFalse.\n",
      "\n",
      "p2_eta\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0318 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00102 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.7306361 > 0.8343464? \tFalse.\n",
      "\n",
      "SPDhits\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.2582 < 0.09? \t\tFalse.\n",
      "CvM metric: \t 0.00069 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.8828318 > 0.8343464? \tTrue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a list of columns that, when used in a logistic regression on\n",
    "# their own, result in all tests being passed\n",
    "lrcv_passed_columns = []\n",
    "for col in ubiquitous_columns:\n",
    "    lrcv = LogisticRegressionCV(random_state=3).fit(train[col].values.reshape(-1,1),\n",
    "                                                    train['signal'])\n",
    "    \n",
    "    print(col)\n",
    "    all_test_passed = evaluate_model(lrcv, [col],\n",
    "                                     validation,\n",
    "                                     check_agreement,\n",
    "                                     check_correlation)\n",
    "    print()\n",
    "    \n",
    "    # agreement and correlation tests passed AND AUC > baseline (the 0.83-etc. value)\n",
    "    if all_test_passed > 0.0:\n",
    "        lrcv_passed_columns.append(col)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST RESULTS\n",
      "KS metric: \t 0.0576 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00103 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.9709629 > 0.8343464? \tTrue.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9709628756266802"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrcv = LogisticRegressionCV(random_state=3)\n",
    "lrcv.fit(train[lrcv_passed_columns], train['signal'])\n",
    "\n",
    "evaluate_model(lrcv, lrcv_passed_columns, validation, check_agreement, check_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's actually surprisingly good for logistic regression. I wonder if, in addition to these variables that passed the three tests in their own logistic regressions, there are other variables that could be added to improve weighted AUC without causing test failure. Let's see if there are any such variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteratively_add_cols(model, base_columns, ubiquitous_columns,\n",
    "                         train, validation, check_agreement, check_correlation):\n",
    "    \"\"\"\n",
    "    Identify candidate columns (those that improve weighted AUC and still pass all tests),\n",
    "    and iteratively add candidate columns until no longer improve AUC or cause test failure\n",
    "    \n",
    "    :param model: initialized model for which the optimal list of columns will be compliled\n",
    "    :param train: X and y together, training data\n",
    "    :param validation: X and y together, validation data\n",
    "    :param check_agreement: ...\n",
    "    :param check_correlation: ...\n",
    "    :param base_columns: list of columns that will definitely be used for the final model\n",
    "    :param ubiquitous_columns: all possible columns that a model could use\n",
    "    \n",
    "    :returns: list of columns from ubquitious columns that includes base_columsn\n",
    "    as well as others that maximize weighted AUC while still passing all tests\n",
    "    \"\"\"\n",
    "\n",
    "    model.fit(train[base_columns], train['signal'])\n",
    "\n",
    "    print('Model Baseline')\n",
    "    # Baseline logistic regression cv AUC by which models with additional columns will be judged\n",
    "    current_auc = evaluate_model(model, base_columns, validation,\n",
    "                                 check_agreement, check_correlation)\n",
    "\n",
    "    candidate_columns = [] \n",
    "    unused_columns = [col for col in ubiquitous_columns if col not in base_columns]\n",
    "    print('Finding candidates to add')\n",
    "    for col in unused_columns:\n",
    "\n",
    "        model.fit(train[base_columns+[col]], train['signal'])\n",
    "\n",
    "        print()\n",
    "        print(col)\n",
    "        extra_column_auc = evaluate_model(model, base_columns+[col], validation,\n",
    "                                          check_agreement, check_correlation)\n",
    "        auc_improvement = extra_column_auc - current_auc\n",
    "\n",
    "        # would only be positive if extra_column_auc is positive AND if adding variable did \n",
    "        # improve regression\n",
    "        if auc_improvement > 0.0:\n",
    "            candidate_columns.append( (col, auc_improvement) )\n",
    "\n",
    "    print(candidate_columns)\n",
    "    # Just to be safe, iteratively add all candidate columns to the model\n",
    "    # but stop when test failure\n",
    "\n",
    "    # list of candidate columns ordered from greatest to least AUC improvement\n",
    "    # https://stackoverflow.com/questions/8459231/sort-tuples-based-on-second-parameter\n",
    "    print('Evaluating candidates')\n",
    "    if candidate_columns is not None:\n",
    "        candidate_columns.sort(key=lambda tupe: tupe[1], reverse=True)\n",
    "        auc_maximizing_columns = [col for col in base_columns]\n",
    "        best_auc_so_far = current_auc # baseline AUC\n",
    "\n",
    "        for col_tupe in candidate_columns:\n",
    "            col = col_tupe[0]\n",
    "\n",
    "            print()\n",
    "            print(col)\n",
    "            model.fit(train[auc_maximizing_columns+[col]], train['signal'])\n",
    "            extra_column_auc = evaluate_model(model, auc_maximizing_columns+[col], validation,\n",
    "                                              check_agreement, check_correlation)\n",
    "            auc_improvement = extra_column_auc - current_auc\n",
    "\n",
    "            # if adding the ith varible doesn't cause tests failure\n",
    "            # AND if it still improves AUC\n",
    "            if auc_improvement > 0.0 and extra_column_auc > best_auc_so_far:\n",
    "                auc_maximizing_columns.append(col)\n",
    "                best_auc_so_far = extra_column_auc\n",
    "\n",
    "    # else there are no candidate columns to evaluate\n",
    "    else:\n",
    "        auc_maximizing_columns = [col for col in base_columns]\n",
    "    print('Done')\n",
    "    \n",
    "    return auc_maximizing_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lrcv_auc_maximizing_columns = iteratively_add_cols(lrcv, lrcv_passed_columns, ubiquitous_columns, \n",
    "#                                                    train, validation, check_agreement, check_correlation)\n",
    "\n",
    "# from a previous iteration of the above\n",
    "lrcv_auc_maximizing_columns = ['dira',\n",
    "                                 'IP',\n",
    "                                 'IPSig',\n",
    "                                 'VertexChi2',\n",
    "                                 'ISO_SumBDT',\n",
    "                                 'p0_IsoBDT',\n",
    "                                 'p1_IsoBDT',\n",
    "                                 'p2_IsoBDT',\n",
    "                                 'p0_track_Chi2Dof',\n",
    "                                 'IP_p0p2',\n",
    "                                 'iso',\n",
    "                                 'p1_track_Chi2Dof',\n",
    "                                 'isolationd',\n",
    "                                 'isolationf',\n",
    "                                 'p2_track_Chi2Dof',\n",
    "                                 'FlightDistanceError',\n",
    "                                 'isolationb',\n",
    "                                 'DOCAone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST RESULTS\n",
      "KS metric: \t 0.0461 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00107 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.9788274 > 0.8343464? \tTrue.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9788273977619022"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see what the improvements were\n",
    "lrcv = LogisticRegressionCV(random_state=3)\n",
    "lrcv.fit(train[lrcv_auc_maximizing_columns], train['signal'])\n",
    "\n",
    "evaluate_model(lrcv, lrcv_auc_maximizing_columns, validation, check_agreement, check_correlation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's an improvement over the logistic regression before those columns were added. Previously the weighted AUROC was 0.97096-ish. This seemingly indicates that the random forest will perform better on the lrcv auc maximizing columns (relative to its performance on lrcv passed columns) as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST RESULTS\n",
      "KS metric: \t 0.0760 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00102 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.9843816 > 0.8343464? \tTrue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# RFC with fewer estimators so that iteratively_add_cols() goes faster\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=3)\n",
    "# check if the random forest can use more columns that the logistic regression\n",
    "# could not use\n",
    "# rfc_auc_maximizing_columns = iteratively_add_cols(rfc, lrcv_auc_maximizing_columns, ubiquitous_columns, \n",
    "#                                                   train, validation, check_agreement, check_correlation)\n",
    "\n",
    "# from a prior run of the above\n",
    "rfc_auc_maximizing_columns = ['dira',\n",
    "                                 'IP',\n",
    "                                 'IPSig',\n",
    "                                 'VertexChi2',\n",
    "                                 'ISO_SumBDT',\n",
    "                                 'p0_IsoBDT',\n",
    "                                 'p1_IsoBDT',\n",
    "                                 'p2_IsoBDT',\n",
    "                                 'p0_track_Chi2Dof',\n",
    "                                 'IP_p0p2',\n",
    "                                 'iso',\n",
    "                                 'p1_track_Chi2Dof',\n",
    "                                 'isolationd',\n",
    "                                 'isolationf',\n",
    "                                 'p2_track_Chi2Dof',\n",
    "                                 'FlightDistanceError',\n",
    "                                 'isolationb',\n",
    "                                 'DOCAone',\n",
    "                                 'p0_IP',\n",
    "                                 'p2_pt',\n",
    "                                 'p1_IP',\n",
    "                                 'p2_eta',\n",
    "                                 'LifeTime',\n",
    "                                 'pt',\n",
    "                                 'p2_IPSig']\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=2000, max_depth=None, random_state=3)\n",
    "rfc.fit(train[lrcv_auc_maximizing_columns], train['signal'])\n",
    "evaluate_model(rfc, lrcv_auc_maximizing_columns, validation, check_agreement, check_correlation)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the final (probability) predictions\n",
    "# result = pandas.DataFrame({'id': test.index})\n",
    "# result['prediction'] = rfc.predict_proba(test[lrcv_auc_maximizing_columns])[:, 1]\n",
    "\n",
    "# result.to_csv('rfc_with_logistic_selection_pt986.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest did well, though not as well as it was expected to do: its weighted AUC was 0.978899. This put it at 6th place out of 15 teams, which is not that bad. At the very least, we have a personal baseline score to beat.\n",
    "\n",
    "Let's keep going and see what a neural network can do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what a neural network can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...on training data\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0445 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00086 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.9865038 > 0.8343464? \tTrue.\n",
      "...on validation data\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0445 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00086 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.9844195 > 0.8343464? \tTrue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "mlpc = MLPClassifier(hidden_layer_sizes=[50],\n",
    "                     activation='tanh',\n",
    "                     solver='adam',\n",
    "                     learning_rate_init=1e-4, # improved AUC by 0.002-ish\n",
    "                     random_state=3)\n",
    "\n",
    "# mlpc_auc_maximizing_columns = iteratively_add_cols(mlpc, lrcv_auc_maximizing_columns, ubiquitous_columns, \n",
    "#                                                    train, validation, check_agreement, check_correlation)\n",
    "\n",
    "# from a previous run of the above\n",
    "mlpc_auc_maximizing_columns = ['dira',\n",
    "                                 'IP',\n",
    "                                 'IPSig',\n",
    "                                 'VertexChi2',\n",
    "                                 'ISO_SumBDT',\n",
    "                                 'p0_IsoBDT',\n",
    "                                 'p1_IsoBDT',\n",
    "                                 'p2_IsoBDT',\n",
    "                                 'p0_track_Chi2Dof',\n",
    "                                 'IP_p0p2',\n",
    "                                 'iso',\n",
    "                                 'p1_track_Chi2Dof',\n",
    "                                 'isolationd',\n",
    "                                 'isolationf',\n",
    "                                 'p2_track_Chi2Dof',\n",
    "                                 'FlightDistanceError',\n",
    "                                 'isolationb',\n",
    "                                 'DOCAone',\n",
    "                                 'FlightDistance',\n",
    "                                 'p0_IPSig',\n",
    "                                 'p2_IPSig',\n",
    "                                 'p1_eta',\n",
    "                                 'p0_eta']\n",
    "\n",
    "mlpc = MLPClassifier(hidden_layer_sizes=[200],\n",
    "                     activation='tanh',\n",
    "                     solver='adam',\n",
    "                     learning_rate_init=1e-4, # improved AUC by 0.002-ish\n",
    "                     random_state=3)\n",
    "mlpc.fit(train[mlpc_auc_maximizing_columns], train['signal'])\n",
    "\n",
    "print('...on training data')\n",
    "evaluate_model(mlpc, mlpc_auc_maximizing_columns, train, check_agreement, check_correlation)\n",
    "\n",
    "print('...on validation data')\n",
    "evaluate_model(mlpc, mlpc_auc_maximizing_columns, validation, check_agreement, check_correlation)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression cv, random forest classifier, and multilayer perceptron each individually seem to perform pretty well. I wonder what happens when their probabilities are averaged. Will that help or hurt the weighted AUROC? Will it cause test failure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST RESULTS\n",
      "KS metric: \t 0.0540 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00100 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.9846739 > 0.8343464? \tTrue.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.98467386646591"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "vc = VotingClassifier(estimators=[('rfc', RandomForestClassifier(n_estimators=50, random_state=3)),\n",
    "                                  ('lrcv', LogisticRegressionCV(random_state=3)),\n",
    "                                  ('mlpc', MLPClassifier(hidden_layer_sizes=[50],\n",
    "                                                         activation='tanh',\n",
    "                                                         solver='adam',\n",
    "                                                         learning_rate_init=1e-4,\n",
    "                                                         random_state=3))],\n",
    "                                  voting='soft')\n",
    "\n",
    "# vc_auc_maximizing_columns = iteratively_add_cols(vc, lrcv_auc_maximizing_columns, ubiquitous_columns, \n",
    "#                                                  train.sample(frac=0.02), validation, check_agreement, check_correlation)\n",
    "\n",
    "# from a previous run of the above\n",
    "vc_auc_maximizing_columns = ['dira',\n",
    "                             'IP',\n",
    "                             'IPSig',\n",
    "                             'VertexChi2',\n",
    "                             'ISO_SumBDT',\n",
    "                             'p0_IsoBDT',\n",
    "                             'p1_IsoBDT',\n",
    "                             'p2_IsoBDT',\n",
    "                             'p0_track_Chi2Dof',\n",
    "                             'IP_p0p2',\n",
    "                             'iso',\n",
    "                             'p1_track_Chi2Dof',\n",
    "                             'isolationd',\n",
    "                             'isolationf',\n",
    "                             'p2_track_Chi2Dof',\n",
    "                             'FlightDistanceError',\n",
    "                             'isolationb',\n",
    "                             'DOCAone',\n",
    "                             'p0_IP']\n",
    "\n",
    "vc = VotingClassifier(estimators=[('rfc', RandomForestClassifier(n_estimators=200, random_state=3)),\n",
    "                                  ('lrcv', LogisticRegressionCV(random_state=3)),\n",
    "                                  ('mlpc', MLPClassifier(hidden_layer_sizes=[200],\n",
    "                                                         activation='tanh',\n",
    "                                                         solver='adam',\n",
    "                                                         learning_rate_init=1e-4,\n",
    "                                                         random_state=3))],\n",
    "                                  voting='soft')\n",
    "\n",
    "vc.fit(train[vc_auc_maximizing_columns], train['signal'])\n",
    "\n",
    "evaluate_model(vc, vc_auc_maximizing_columns, validation, check_agreement, check_correlation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that averaging the predicted probabilities helped slightly (with an emphasis on \"slightly\"). Overall, though, I'm not sure if averaging their predicted probabilities will reduce bias or introduce bias. \n",
    "\n",
    "Let's try the support vector classifier (SVC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] C=0.7, kernel=rbf, tol=0.0001 ...................................\n",
      "[CV]  C=0.7, kernel=rbf, tol=0.0001, score=0.856628674265147, total=   1.8s\n",
      "[CV] C=0.7, kernel=rbf, tol=0.0001 ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.7, kernel=rbf, tol=0.0001, score=0.8485302939412117, total=   1.7s\n",
      "[CV] C=0.7, kernel=rbf, tol=0.0001 ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    5.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.7, kernel=rbf, tol=0.0001, score=0.8427370948379351, total=   1.8s\n",
      "[CV] C=0.7999999999999999, kernel=rbf, tol=0.0001 ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    7.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.7999999999999999, kernel=rbf, tol=0.0001, score=0.8569286142771446, total=   1.6s\n",
      "[CV] C=0.7999999999999999, kernel=rbf, tol=0.0001 ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    9.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.7999999999999999, kernel=rbf, tol=0.0001, score=0.847630473905219, total=   1.7s\n",
      "[CV] C=0.7999999999999999, kernel=rbf, tol=0.0001 ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   12.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.7999999999999999, kernel=rbf, tol=0.0001, score=0.8406362545018007, total=   1.7s\n",
      "[CV] C=0.8999999999999999, kernel=rbf, tol=0.0001 ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:   14.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.8999999999999999, kernel=rbf, tol=0.0001, score=0.8554289142171566, total=   1.8s\n",
      "[CV] C=0.8999999999999999, kernel=rbf, tol=0.0001 ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:   17.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.8999999999999999, kernel=rbf, tol=0.0001, score=0.8485302939412117, total=   1.7s\n",
      "[CV] C=0.8999999999999999, kernel=rbf, tol=0.0001 ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:   19.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.8999999999999999, kernel=rbf, tol=0.0001, score=0.8403361344537815, total=   1.6s\n",
      "[CV] C=0.9999999999999999, kernel=rbf, tol=0.0001 ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:   22.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.9999999999999999, kernel=rbf, tol=0.0001, score=0.855128974205159, total=   1.7s\n",
      "[CV] C=0.9999999999999999, kernel=rbf, tol=0.0001 ....................\n",
      "[CV]  C=0.9999999999999999, kernel=rbf, tol=0.0001, score=0.8494301139772046, total=   1.7s\n",
      "[CV] C=0.9999999999999999, kernel=rbf, tol=0.0001 ....................\n",
      "[CV]  C=0.9999999999999999, kernel=rbf, tol=0.0001, score=0.842436974789916, total=   1.7s\n",
      "[CV] C=1.0999999999999999, kernel=rbf, tol=0.0001 ....................\n",
      "[CV]  C=1.0999999999999999, kernel=rbf, tol=0.0001, score=0.855128974205159, total=   1.7s\n",
      "[CV] C=1.0999999999999999, kernel=rbf, tol=0.0001 ....................\n",
      "[CV]  C=1.0999999999999999, kernel=rbf, tol=0.0001, score=0.8497300539892022, total=   1.7s\n",
      "[CV] C=1.0999999999999999, kernel=rbf, tol=0.0001 ....................\n",
      "[CV]  C=1.0999999999999999, kernel=rbf, tol=0.0001, score=0.8418367346938775, total=   1.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:   36.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best estimator SVC(C=0.7, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.0001, verbose=False)\n",
      "best score 0.8493\n",
      "TEST RESULTS\n",
      "KS metric: \t 0.0801 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00098 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.9813031 > 0.8343464? \tTrue.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9813031429748271"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# These values were narrowed down by previous grid searches\n",
    "svc_param_dict = {'C': numpy.arange(0.7, 1.2, 0.1),\n",
    "                  'kernel': ['rbf'],\n",
    "                  'tol': [1e-4]}\n",
    "\n",
    "svc_gridsearchcv = GridSearchCV(estimator=SVC(), \n",
    "                                param_grid=svc_param_dict,\n",
    "                                verbose=10)\n",
    "\n",
    "svc_ss = StandardScaler().fit(pandas.concat([train[lrcv_auc_maximizing_columns],\n",
    "                                             validation[lrcv_auc_maximizing_columns],]))\n",
    "\n",
    "num_samples = 10000\n",
    "sample_indicies = train.sample(n=num_samples).index\n",
    "\n",
    "# easier \n",
    "svc_gridsearchcv.fit(svc_ss.transform(train[lrcv_auc_maximizing_columns].iloc[sample_indicies].values),\n",
    "                     train['signal'].iloc[sample_indicies])\n",
    "\n",
    "print('best estimator', svc_gridsearchcv.best_estimator_)\n",
    "print('best score', svc_gridsearchcv.best_score_)\n",
    "\n",
    "svc_pipeline = Pipeline( [('ss', StandardScaler()),\n",
    "                          ('svc', SVC(C=1.0,\n",
    "                                      kernel='rbf',\n",
    "                                      tol=1e-4,\n",
    "                                      probability=True))] ) # prob=True slows training down considerably\n",
    "# according to scikit learn page and just running this once\n",
    "\n",
    "svc_pipeline.fit(train[lrcv_auc_maximizing_columns], train['signal'])\n",
    "evaluate_model(svc_pipeline, lrcv_auc_maximizing_columns, validation, check_agreement, check_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the support vector classifier fails the agreement test and given that its AUC is not much better than the random forest, there is no advantage in keeping it alone. Perphaps if it is part of a voting classifier it would not affect the voting classifier such that the voting classifier would fail tests. In that case, perhaps the SVC would provide some marginal benefit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST RESULTS\n",
      "KS metric: \t 0.0746 < 0.09? \t\tTrue.\n",
      "CvM metric: \t 0.00096 < 0.002? \t\tTrue.\n",
      "AUC: \t\t 0.9852558 > 0.8343464? \tTrue.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9852557964676435"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "vc = VotingClassifier(estimators=[('rfc', RandomForestClassifier(n_estimators=50, random_state=3)),\n",
    "                                  ('lrcv', LogisticRegressionCV(random_state=3)),\n",
    "                                  ('mlpc', MLPClassifier(hidden_layer_sizes=[50],\n",
    "                                                         activation='tanh',\n",
    "                                                         solver='adam',\n",
    "                                                         learning_rate_init=1e-4,\n",
    "                                                         random_state=3)),\n",
    "                                 ('svc_pipeline',Pipeline( [('ss', StandardScaler()),\n",
    "                                                             ('svc', SVC(C=1.0,\n",
    "                                                                          kernel='rbf',\n",
    "                                                                          tol=1e-4,\n",
    "                                                                          probability=True))] ))],\n",
    "                                  voting='soft')\n",
    "# vc_auc_maximizing_columns = iteratively_add_cols(vc, lrcv_auc_maximizing_columns, ubiquitous_columns, \n",
    "#                                                  train.sample(n=2000), validation, check_agreement, check_correlation)\n",
    "\n",
    "# from a previous run of the above\n",
    "vc_auc_maximizing_columns = ['dira',\n",
    "                             'IP',\n",
    "                             'IPSig',\n",
    "                             'VertexChi2',\n",
    "                             'ISO_SumBDT',\n",
    "                             'p0_IsoBDT',\n",
    "                             'p1_IsoBDT',\n",
    "                             'p2_IsoBDT',\n",
    "                             'p0_track_Chi2Dof',\n",
    "                             'IP_p0p2',\n",
    "                             'iso',\n",
    "                             'p1_track_Chi2Dof',\n",
    "                             'isolationd',\n",
    "                             'isolationf',\n",
    "                             'p2_track_Chi2Dof',\n",
    "                             'FlightDistanceError',\n",
    "                             'isolationb',\n",
    "                             'DOCAone',\n",
    "                             'pt',\n",
    "                             'IP_p1p2',\n",
    "                             'p1_eta',\n",
    "                             'p2_IPSig']\n",
    "\n",
    "vc = VotingClassifier(estimators=[('rfc', RandomForestClassifier(n_estimators=200, random_state=3)),\n",
    "                                  ('lrcv', LogisticRegressionCV(random_state=3)),\n",
    "                                  ('mlpc', MLPClassifier(hidden_layer_sizes=[200],\n",
    "                                                         activation='tanh',\n",
    "                                                         solver='adam',\n",
    "                                                         learning_rate_init=1e-4,\n",
    "                                                         random_state=3)),\n",
    "                                 ('svc_pipeline',Pipeline( [('ss', StandardScaler()),\n",
    "                                                            ('svc', SVC(C=1.0,\n",
    "                                                                        kernel='rbf',\n",
    "                                                                        tol=1e-4,\n",
    "                                                                        probability=True))] ))],\n",
    "                      voting='soft')\n",
    "\n",
    "vc.fit(train[vc_auc_maximizing_columns], train['signal'])\n",
    "\n",
    "evaluate_model(vc, vc_auc_maximizing_columns, validation, check_agreement, check_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit surprisingly, the SVC that failed the agreement test on its, when included in the voting classifier, improved AUC! It improved AUC slightly over the 3-model voting classifier (by about 0.0009), and the voting classifier still passed all of the tests, though the agreement test (KS metric) is a bit closer to the max than I would like. We'll go with this model as the next model to submit to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the final (probability) predictions\n",
    "# result = pandas.DataFrame({'id': test.index})\n",
    "\n",
    "# print('making probability predictions')\n",
    "# result['prediction'] = vc.predict_proba(test[lrcv_auc_maximizing_columns])[:, 1]\n",
    "\n",
    "# print('writing to csv')\n",
    "# result.to_csv('vc_with_logistic_selection_pt9876.csv', index=False, sep=',')\n",
    "# print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, that voting classifier with the support vector classifier performed worse than expected. It scored a weighted AUC score of 0.980854 on Kaggle's public leaderboard. While this is an improvement over the random forest by itself, it is not comparable to the validation weighted AUC score: 0.9853.\n",
    "\n",
    "Given all of the discrepancies between the expected scores and the actual, public leaderboard scores, what is needed most is cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "\n",
    "def auc_confidence_interval_calc(model, df_with_x_and_y, input_cols, target_col, alpha=0.99, validation_size=0.997):\n",
    "    \"\"\"\n",
    "    Print alpha*100% confidence interval for a model's weighted AUC.\n",
    "    \n",
    "    :param model: initialized but not fitted classification model\n",
    "    :param alpha: alpha for the confidence interval (default is 0.99, so gives 99% CI)\n",
    "    :param df_with_x_and_y: pandas dataframe with inputs and target cols together\n",
    "    :param input_cols: list of input columns\n",
    "    :param target_cols: string that is the column name of the label column\n",
    "    :param validation_size: size of validation split; proportion of df_with_x_and_y that\n",
    "    will be used to obtain AUC score  (0.997 gets 99% CI's containing the 2 Kaggle public\n",
    "    leaderboard scores from the rfc and vc models: 0.978899 and 0.980854 respectively. \n",
    "    Moreover, the avereage is roughly the same as the public leaderboard score.)\n",
    "    \n",
    "    :returns: list of each validation AUC score for each iteration\n",
    "    \"\"\"\n",
    "    \n",
    "    # 0.3%=train/99.7%=test\n",
    "    # can you calculate that test_size percentage?\n",
    "    # plus, confidence intervals?\n",
    "    # plus, less is more#\n",
    "    # formula for test_size?\n",
    "    ss = ShuffleSplit(n_splits=20, test_size=validation_size) # based on 1st rfc public score\n",
    "\n",
    "    val_auc_scores = []\n",
    "    iteration = 1\n",
    "    for train_indicies, val_indicies in ss.split(df_with_x_and_y[input_cols], df_with_x_and_y[target_col]):\n",
    "        model.fit(df_with_x_and_y[input_cols].iloc[train_indicies],\n",
    "                  df_with_x_and_y[target_col].iloc[train_indicies])\n",
    "\n",
    "            # Weighted AUC on validation data\n",
    "        validation_probs = model.predict_proba(df_with_x_and_y[input_cols].iloc[val_indicies])[:, 1]\n",
    "        val_auc = roc_auc_truncated(df_with_x_and_y[target_col].iloc[val_indicies], validation_probs)\n",
    "\n",
    "        val_auc_scores.append(val_auc)\n",
    "        print('AUC on iteration {0}: {1:.7f}'.format(iteration, val_auc))\n",
    "        iteration += 1\n",
    "\n",
    "    print()\n",
    "    avg_auc = numpy.mean(val_auc_scores)\n",
    "    print('Average AUC score: {0:.7f}'.format(avg_auc))\n",
    "\n",
    "    sdev_auc = numpy.std(val_auc_scores)\n",
    "    print('Standard deviation of AUC scores: {0:.7f}'.format(sdev_auc))\n",
    "\n",
    "    # Assume t-distribution for distribution \n",
    "    # https://stackoverflow.com/questions/28242593/correct-way-to-obtain-confidence-interval-with-scipy\n",
    "    ci = stats.t.interval(df=len(val_auc_scores)-1,\n",
    "                              alpha=alpha,\n",
    "                              loc=avg_auc,\n",
    "                              scale=(sdev_auc / math.sqrt(len(val_auc_scores))), )\n",
    "    print('{}% Confidence interval for average AUC score:   {}'.format(alpha*100.0, ci))\n",
    "    \n",
    "    return val_auc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on iteration 1: 0.9790934\n",
      "AUC on iteration 2: 0.9794377\n",
      "AUC on iteration 3: 0.9787227\n",
      "AUC on iteration 4: 0.9782271\n",
      "AUC on iteration 5: 0.9771244\n",
      "AUC on iteration 6: 0.9774839\n",
      "AUC on iteration 7: 0.9777520\n",
      "AUC on iteration 8: 0.9785983\n",
      "AUC on iteration 9: 0.9754350\n",
      "AUC on iteration 10: 0.9775475\n",
      "AUC on iteration 11: 0.9781546\n",
      "AUC on iteration 12: 0.9792799\n",
      "AUC on iteration 13: 0.9790953\n",
      "AUC on iteration 14: 0.9769444\n",
      "AUC on iteration 15: 0.9794860\n",
      "AUC on iteration 16: 0.9787367\n",
      "AUC on iteration 17: 0.9774344\n",
      "AUC on iteration 18: 0.9782153\n",
      "AUC on iteration 19: 0.9788952\n",
      "AUC on iteration 20: 0.9765295\n",
      "\n",
      "Average AUC score: 0.9781097\n",
      "Standard deviation of AUC scores: 0.0010462\n",
      "99.0% Confidence interval for average AUC score:   (0.9774404059016386, 0.9787789212211475)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_train = pandas.read_csv('training.csv', index_col='id') # data on which to train models\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=200)\n",
    "auc_confidence_interval_calc(rfc, all_train, lrcv_auc_maximizing_columns, 'signal')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on iteration 1: 0.9811112\n",
      "AUC on iteration 2: 0.9814783\n",
      "AUC on iteration 3: 0.9794880\n",
      "AUC on iteration 4: 0.9776859\n",
      "AUC on iteration 5: 0.9810987\n",
      "AUC on iteration 6: 0.9826247\n",
      "AUC on iteration 7: 0.9825912\n",
      "AUC on iteration 8: 0.9818502\n",
      "AUC on iteration 9: 0.9746417\n",
      "AUC on iteration 10: 0.9820130\n",
      "AUC on iteration 11: 0.9812954\n",
      "AUC on iteration 12: 0.9798816\n",
      "AUC on iteration 13: 0.9805154\n",
      "AUC on iteration 14: 0.9821510\n",
      "AUC on iteration 15: 0.9806148\n",
      "AUC on iteration 16: 0.9798040\n",
      "AUC on iteration 17: 0.9799202\n",
      "AUC on iteration 18: 0.9828073\n",
      "AUC on iteration 19: 0.9796643\n",
      "AUC on iteration 20: 0.9770145\n",
      "\n",
      "Average AUC score: 0.9804126\n",
      "Standard deviation of AUC scores: 0.0020068\n",
      "99.0% Confidence interval for average AUC score:   (0.979128787118357, 0.9816963676078468)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vc = VotingClassifier(estimators=[('rfc', RandomForestClassifier(n_estimators=200, random_state=3)),\n",
    "                                  ('lrcv', LogisticRegressionCV(random_state=3)),\n",
    "                                  ('mlpc', MLPClassifier(hidden_layer_sizes=[200],\n",
    "                                                         activation='tanh',\n",
    "                                                         solver='adam',\n",
    "                                                         learning_rate_init=1e-4,\n",
    "                                                         random_state=3)),\n",
    "                                 ('svc_pipeline',Pipeline( [('ss', StandardScaler()),\n",
    "                                                             ('svc', SVC(C=1.0,\n",
    "                                                                          kernel='rbf',\n",
    "                                                                          tol=1e-4,\n",
    "                                                                          probability=True))] ))],\n",
    "                                  voting='soft')\n",
    "\n",
    "auc_confidence_interval_calc(vc, all_train, vc_auc_maximizing_columns, 'signal')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning the function auc_confidence_interval_calc(), it now gives a 99% confidence interval for the average AUC score. That average AUC score, at least for the random forest classifier (rfc) and the 4-model voting classifier (vc), is roughly the same as the Kaggle public leaderboard score. This will be a very useful tool to have when evaluating future models. I wonder how the other, 3-model voting classifier measures up to the 4-model voting classifier based on this function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on iteration 1: 0.9790614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on iteration 2: 0.9801280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on iteration 3: 0.9753712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on iteration 4: 0.9787525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on iteration 5: 0.9805673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on iteration 6: 0.9795239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on iteration 7: 0.9779568\n",
      "AUC on iteration 8: 0.9781937\n",
      "AUC on iteration 9: 0.9784091\n",
      "AUC on iteration 10: 0.9791098\n",
      "AUC on iteration 11: 0.9792499\n",
      "AUC on iteration 12: 0.9795108\n",
      "AUC on iteration 13: 0.9789616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on iteration 14: 0.9825593\n",
      "AUC on iteration 15: 0.9790026\n",
      "AUC on iteration 16: 0.9798670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on iteration 17: 0.9780733\n",
      "AUC on iteration 18: 0.9768961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on iteration 19: 0.9804116\n",
      "AUC on iteration 20: 0.9779095\n",
      "\n",
      "Average AUC score: 0.9789758\n",
      "Standard deviation of AUC scores: 0.0014411\n",
      "99.0% Confidence interval for average AUC score:   (0.9780538474227752, 0.9798976980629083)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vc = VotingClassifier(estimators=[('rfc', RandomForestClassifier(n_estimators=200, random_state=3)),\n",
    "                                  ('lrcv', LogisticRegressionCV(random_state=3)),\n",
    "                                  ('mlpc', MLPClassifier(hidden_layer_sizes=[200],\n",
    "                                                         activation='tanh',\n",
    "                                                         solver='adam',\n",
    "                                                         learning_rate_init=1e-4,\n",
    "                                                         random_state=3))],\n",
    "                                  voting='soft')\n",
    "auc_confidence_interval_calc(vc, all_train, lrcv_auc_maximizing_columns, 'signal')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only does the 3-model one seem like it would be worse than the 4-model one, it would also not be much better than the random forest classifier on its own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although these tiles were cut, I did try principal componenet and linear discriminant analysis with a neural network, random forest, and SVC. I also tried AdaBoost, GradientBoost, and DecisionTree classifiers by themselves and within the voting classifier. For the voting classifier, I also tried to mess with the model weights, but unfortunately, still, nothing really improved the weighted AUROC. \n",
    "\n",
    "Because of this and in the interest of moving on to another competition, this is where I will leave off on this physics competition. The best model, the voting classifier, scored 0.980854\n",
    "for its weighted AUC, thereby placing it at, currently, 5th out of 20 teams (top 25%)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
