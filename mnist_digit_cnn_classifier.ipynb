{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset contains tens of thousands of 28px by 28px images of hand-written digits. Handwriting varies from person to person, so building a generalizable machine learning model that can very accurately identify hand written digits, which is the objective here, can be a bit of a challenge. \n",
    "\n",
    "On top of that, there is an issue with the sheer number of input variables: 28 \\* 28 = 784 input pixels. If a fully-connected neural network with an equal number of nodes in its first layer is utlilized to identify the digits, then there would be 784 inputs * 784 nodes = 614,656 weights to learn in just the first layer alone. This makes utilizing regular neural networks computationally expensive.\n",
    "\n",
    "Fortunately, there is an alternative: convolutional neural networks. Convolutional neural networks reduce the number of weights for a fully connected neural network by convolution and by pooling. Fairly good explanations of the two processes can be found in these two YouTube videos: https://youtu.be/NVH8EYPHi30 and https://youtu.be/umGJ30-15_A. The rough idea is that this by convolution and pooling, patches of the image are summarized. What goes into a summary is something the is learned by the network itself. By summarizing, the image is, in a way, \"compressed.\" The \"compressed\" image is then fed into some classification algorithm (typically a regular neural network) that does the actual classification. \n",
    "\n",
    "There are many kinds of convolutional neural networks, especially those geared towards accurately identifying MNSIT digits. However, the classic version will be utilized here given that this is my first serious look at machine learning for identifying things in images. The network in this notebook will be based off of the ones found here: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py and https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "train = pandas.read_csv('train.csv')\n",
    "test = pandas.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21322</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17842</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27557</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31601</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "1386       9       0       0       0       0       0       0       0       0   \n",
       "21322      5       0       0       0       0       0       0       0       0   \n",
       "17842      2       0       0       0       0       0       0       0       0   \n",
       "27557      1       0       0       0       0       0       0       0       0   \n",
       "31601      2       0       0       0       0       0       0       0       0   \n",
       "\n",
       "       pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "1386        0    ...            0         0         0         0         0   \n",
       "21322       0    ...            0         0         0         0         0   \n",
       "17842       0    ...            0         0         0         0         0   \n",
       "27557       0    ...            0         0         0         0         0   \n",
       "31601       0    ...            0         0         0         0         0   \n",
       "\n",
       "       pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "1386          0         0         0         0         0  \n",
       "21322         0         0         0         0         0  \n",
       "17842         0         0         0         0         0  \n",
       "27557         0         0         0         0         0  \n",
       "31601         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just out of curiosity, I wonder what the handwritten digits look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA5UlEQVR4nGNgGGSAEZnDZxnBoHKHYdN6THXMyQ//HDq1bN6jN4d4MCQl/221Y2BgYGBQWMyPRTILyhCBijAhJFUgFEfT+XhMO9lPH+BnYOA++68Pm8s1fs5mkD3wLoEJmyTDrB9FnR9tsUoxMDBs/fceWQ7ViP0Mvz/i0shy5Ne5+6w4JMv/9St9mIRdjv/MY2WGrp9CWCUb/iczMHDeTMYquf29PAMDg98ObHJiP4oZGBiYe04hHIiQZGb7zsDAYFKUChdBDyrL7W9WY9P557u3hIvOwcRPWB1U/Pvf7gzMREB1AABEMkK9bSVPVgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x111F25400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAr0lEQVR4nGNgGDaAkYGBQaCNV+GByc2foYyvdyo8YGDYvOsDQpJlpSQD438GBgbG/w8UGBiNWCxP4TTs9V8zKIsJQ06N4+8fnBoP/92BU876/z8VGBvD2OD/O+7j0ijw+Z89TlMT/97jwiUn/e5vKU6N9v/+4ZRj2Pz3JE45mde/vHFKmv99jcxF9acjwz7cVp75F45bp/z/+7glGRh4cRu75O913DrXMmzCrRMNAAAx5DMywgM4twAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x111F25470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAApElEQVR4nM3RIQ7CQBQE0IGgwCBQkGAIR+AMpRbHERrCDTgCV+AIJJwADxbRVBZbhcFMZovdJjtIwld/92Vnf/KBP6it1h4rZo5GxzefDnOSZ4d3MooddHEVgJ7BIgSgTYeOryJPwzTuKTabtM0eFEsz6lJqtYguOgMFIMTnftRPATQ0sRWpnbHiRd7macslybzDhaT7ECJZf0O75wPrbOLwd/UBVRFGqCG8XiwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x10D27CB38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA4klEQVR4nGNgGEigu+L/v91zk7FLxvz9+/fv34cqSEJMSOylWvyHZZYiCbDAWasf33nK8JeBDbfFe/8+VMRuLAMDAwODjDEuSTExFC6KZMoxLZwWpnz++/fv39t5WHV++cvAsC2Ut1wSq9bIhY5CDN1/E3GazaDw+pI3VtcyMDAwPFigLYtTkmEVblMZ2Nb/3Q1lQsOWh5/h23sGBgYGBtYqP4Z7qMrT//y9kc3CwMCgsPLv7zvaaIal5T39uzibj6/p75eLWOzSKn309+/fP/fVEUKMSNLqIYr/j8/D41Z6AAB8H0ZY9B33WgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x10D27CB38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAuElEQVR4nGNgGBmAkYGBgd36+RcGBgaeFIZzhxgYkn0MV0cgFDT/+fP3DzKOhUowYTPuNpLk3BcMDAznGBi2HvpCwBE+v/4/kcdl7Nuf/z7x4NIZ+udvCC45qUt/nnDCOCxokvLaDJ++45KU/M8QgMtUrqN/liB4aK4VNmd4jlPy+XZGRpySRp7//+OyUu3pn13yuCQL//5xQeKieoWRkQE57FHtlMBtIwPDw7+reHHqtGC4/RmJCwAP7EGFOMl29gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x10D27CB38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA7klEQVR4nGNgoBJQefRQGafkyf+P1XFIqc3480gbl76wf//acRp65l8XGy65hl//THBqvP/vCapTmRBMeS6GxXdxaUz+t4sTVQRJZyrD+++okixwlr8BhNYWDL9+5hSapDQbwyIdvTgGY2EGhmcyaFZm/fu39+O///++Xrjx77snmk4RBgZHhofXFrzba3Po2y00nW/+/TseLczAwCD38N8TdJ/M/vcvjYGBgYGt/d/vTnRJ7Q//7mizsIm0//s3AV2OgWH2v3//lq399+/fQw1MSb0Xv/79+/fv1xx4dDMiS2sFOe9lWI4zfAc5AADi8Vjs16XxCAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x10D27CB38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA6ElEQVR4nGNgoD349P/v37/LVbFLfvz758+fP3fVYXxGZMkYFgYG8Wqu1ZE4zU79sxzGZMKQ/IpgYkjy5eOR9DZmuInLRtuTf/4YYJUxXfP0858/baxYJd/8/fPnzx8brHJWP/7//fvj721JbA76+unf3zkRPxTFYQIsSJIXXYy+r5R/I43LsQwMhkiuherkMWQ4/wUqdOYeqnLuJX/+nJnMDmFFoJkl+eDPnz93BBgYGNqQvQIx9vnsRgYG4ZivjC6eDGfOojtDZuWfP3/+/Pn7588FIUxHMuut+fPnz98/Gw1xe2TgAQDd0FyLUTTHmwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x10D27CB38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABJUlEQVR4nM3QvyvEcRzH8afz4wwSFj8GBj9KkgXDGcxnOsliJoMuA6duYvMnEINJDnEm+ZWTlBukLquBKL/TiU7q9T7Dhe+dvqPynt69H5/3jz7w51E4edjgiqN66XSz5uR1i5t1PGvBdeiaPVW42aClZ77y0q6cZ5s6KQLA6z+40FG50/pTmgKgcVkyKeTEqJ5aAdr2ldoI3WbQk7HiWtbPAILdbAfiytoYMJsH6EvbLKTNnGPnJD9QcGyxEjAlqh1jzyEJDHey+so0H0M3js4JyQdcaSuf9ndbzFQ9Wau9rKhuqfBuLKta9mi9wIOF63fMmrL/h4jiHTCit1t9hHOMqoQeI8FdmTSea1BzapJk9+M/Z+R9Z5W+Hgb2orHL343/Jj4BjFV2El4gq+wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x10D27CB38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA40lEQVR4nGNgGGBgbzBhwq9tWczY5EIu/fv74O/fv8nYJPf8PeYrqdP5dz1MgAVJ8oC2/W+GzwoMVjjtnfv3bzMuubLff//qYpcSnf7r7+9SJqxyIpf//v1Vgl2fxOl/f/++k8Yu6ff339+/fy94wgUYkSSV1vL/ZxDiZ5B4hcu5mgf+isHYGC67/gyXNgYGhoBfCJ3oIOrKX1ySPGVf/v7dwYomys3CwMDAabPn79+/j9XgotBYeXHqHgODpyTTP4YzrbfQzbv09+/fv3//ftnQgiUZCGZu/fv3Tr8sHm/QCQAAc35XaM/XVbUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x10D27CB38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAgklEQVR4nNWRQQ7CQAwDR6gPCz8LL3N5mTlUacumizhwIadVxk5iLfyg5E/MMbRu9YhgHfXLDuE+nWpn69XYcd338KjsC7KCyG6ZbJeo04K2FTV4eZOQsPa4mzNscWSuax+QEPAEXVvDNurnylbd2r5nCzhhxM6ysbP3CgKpifEP6gWV72q3Z+hrzwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x10D27CEB8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Helped: https://stackoverflow.com/questions/2659312/how-do-i-convert-a-numpy-array-to-and-display-an-image\n",
    "from PIL import Image\n",
    "\n",
    "def display_mnist_digit(digit_array):\n",
    "    \"\"\"\n",
    "    Display the digit in the MNIST data set.\n",
    "    \n",
    "    :param digit_array: 1D numpy array of the data\n",
    "    \"\"\"\n",
    "    image_array = digit_array.astype(np.uint8)\n",
    "    img = Image.fromarray(image_array.reshape(28, 28))\n",
    "\n",
    "    display(img)\n",
    "    \n",
    "num_image_to_display = 10\n",
    "for idx in range(222, 222+num_image_to_display):\n",
    "    display_mnist_digit(train.drop('label', axis=1).iloc[idx].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plan is to learn how to use a convolution neural network (CNN); however, I first want to use a regular, fully connected neural network (a multilayer perceptron). Hopefully it will give us a baseline by which the CNN can be judged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9594444444444444"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(train.drop('label', axis=1),\n",
    "                                                    train['label'], \n",
    "                                                    test_size=0.25)\n",
    "\n",
    "mlpc = MLPClassifier(hidden_layer_sizes=[1400]) #1400 nodes\n",
    "\n",
    "mlpc.fit(x_train, y_train)\n",
    "\n",
    "accuracy_score(y_test, mlpc.predict(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's actually not an absolutely terrible baseline. If this accuracy score is accurate, then this model would place at about 2153rd place out of 2646 teams: ahead of about 20% of the all models. Again, though, this is just a baseline; the goal is to build a CNN to classify images.\n",
    "\n",
    "The images are currently represented as 1D arrays but a CNN, typically, expects a 2D array. The reason for this has to do with the convolution part in the covolutional neural network. The actual process of convolution is covered well in these two YouTube videos: https://youtu.be/NVH8EYPHi30 and https://youtu.be/umGJ30-15_A. The basic idea is that convolution takes advantage of spatial relationships (pixels near each other are probably related) and sort of summarizes patches of an image.\n",
    "\n",
    "Let's convert the images into 2D arrays. This will, inevitably, take up more memory: at least 1 bytes per 8-bit int * 784 pixels per image * 42,000 images = 32,928,000 or 32.928MB. The alternative, however, is to do this conversion as part of a pipeline, which, if the pipeline is run more than once, would not be efficient speed-wise, only memory-wise. \n",
    "\n",
    "The 32.928MB number, at least on my machine with 4GB of RAM, is not too large. For other data sets with hundreds of thousands of images that each have millions of pixels (real world data sets), converting everything at once would be more difficult. For example, a data set with 100,000 black-and-white images each with 1280 * 720 pixels (a 720p image) would be at least 92,160,000,000 bytes or 92.16 GB. \n",
    "\n",
    "Therefore, by converting to and then saving 2D representations of the mnist digit images for the CNN once at the start, we can tune the CNN later without having to redo this conversion and without trading-off all of our RAM/memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful dummy encoding?: True.\n",
      "Index(['label_0', 'label_1', 'label_2', 'label_3', 'label_4', 'label_5',\n",
      "       'label_6', 'label_7', 'label_8', 'label_9'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 4D array because \n",
    "# 1st dim = image index\n",
    "# 2nd and 3rd dims = width and height (column and row coordinates of a pixel in an image)\n",
    "# 4th dim = channel(s) (RGB or luminance if only gray scale)\n",
    "# shape: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
    "x = train.drop('label', axis=1).values.reshape(train.shape[0], 28, 28, 1)\n",
    "y = train['label']\n",
    "\n",
    "# Want to be able to use categorical cross entropy as a loss\n",
    "# measurement, which requires dummy encoding train columns\n",
    "y_binary = pandas.get_dummies(pandas.DataFrame([str(label) for label in y],\n",
    "                                               columns=['label']))\n",
    "\n",
    "# put columns in sorted order (0 at lowest idx; 9 at highest idx)\n",
    "y_binary = y_binary[ sorted(y_binary.columns) ]\n",
    "\n",
    "print('Successful dummy encoding?: {}.'.format(y_binary.shape[1]==10))\n",
    "print(y_binary.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x,\n",
    "                                                  y_binary,\n",
    "                                                  test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard convolutional neural network has two parts. The first part contains a convolutional layer, then an activation layer (like the rectified linear unit (ReLU) that is max{0, x}), and then a pooling layer (like a max pooling layer that keeps only the max value in a patch of convoluted pixels) to reduce dimensionality. This set, convolution then activation then pooling, can be repeated multiple times.\n",
    "\n",
    "In the second section, the final pooling outputs are fed into a fully connected network that actually makes the classification. The first part can be though of as extracting spatial features while the second part can be though of as taking those features and making some classification. This model will draw upon: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "39900/39900 [==============================] - 434s 11ms/step - loss: 1.4987 - categorical_accuracy: 0.1945\n",
      "Epoch 2/15\n",
      "39900/39900 [==============================] - 481s 12ms/step - loss: 1.1106 - categorical_accuracy: 0.3434\n",
      "Epoch 3/15\n",
      "39900/39900 [==============================] - 490s 12ms/step - loss: 0.0294 - categorical_accuracy: 0.9587\n",
      "Epoch 4/15\n",
      "39900/39900 [==============================] - 477s 12ms/step - loss: 0.0148 - categorical_accuracy: 0.9801\n",
      "Epoch 5/15\n",
      "39900/39900 [==============================] - 495s 12ms/step - loss: 0.0109 - categorical_accuracy: 0.9852\n",
      "Epoch 6/15\n",
      "39900/39900 [==============================] - 467s 12ms/step - loss: 0.0085 - categorical_accuracy: 0.9895\n",
      "Epoch 7/15\n",
      "39900/39900 [==============================] - 471s 12ms/step - loss: 0.0072 - categorical_accuracy: 0.9907\n",
      "Epoch 8/15\n",
      "39900/39900 [==============================] - 483s 12ms/step - loss: 0.0061 - categorical_accuracy: 0.9927\n",
      "Epoch 9/15\n",
      "39900/39900 [==============================] - 517s 13ms/step - loss: 0.0052 - categorical_accuracy: 0.9936\n",
      "Epoch 10/15\n",
      "39900/39900 [==============================] - 512s 13ms/step - loss: 0.0043 - categorical_accuracy: 0.9949\n",
      "Epoch 11/15\n",
      "39900/39900 [==============================] - 474s 12ms/step - loss: 0.0038 - categorical_accuracy: 0.9956\n",
      "Epoch 12/15\n",
      "39900/39900 [==============================] - 497s 12ms/step - loss: 0.0035 - categorical_accuracy: 0.9961\n",
      "Epoch 13/15\n",
      "39900/39900 [==============================] - 495s 12ms/step - loss: 0.0033 - categorical_accuracy: 0.9962\n",
      "Epoch 14/15\n",
      "39900/39900 [==============================] - 531s 13ms/step - loss: 0.0030 - categorical_accuracy: 0.9968\n",
      "Epoch 15/15\n",
      "39900/39900 [==============================] - 490s 12ms/step - loss: 0.0027 - categorical_accuracy: 0.9971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x158b26748>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# PART 1\n",
    "# CONVOLUTION LAYER 1\n",
    "model.add(Conv2D(96, \n",
    "                 kernel_size=(4,4),  # convolution window size: hxw\n",
    "                 strides = (1,1), # how far move window after each convolution\n",
    "                 input_shape=(28, 28, 1), # image w, image h, num channels/colors\n",
    "                 data_format=\"channels_last\") # channel=colors (RGB or luminance)\n",
    "         )\n",
    "model.add(Activation('relu')) # rectified linear unit\n",
    "# model.add(MaxPooling2D(pool_size=(2,2))) # 2x2 patches become 1x1's \n",
    "\n",
    "# CONVOLUTION LAYER 2\n",
    "model.add(Conv2D(160, kernel_size=(3,3), strides=(1,1)))\n",
    "model.add(Activation('relu')) # rectified linear unit\n",
    "model.add(MaxPooling2D(pool_size=(2,2))) # 2x2 patches become 1x1's \n",
    "model.add(Dropout(0.28)) # rand set frac of inputs to 0 - improve generalizability\n",
    "\n",
    "# CONVOLUTION LAYER 3\n",
    "model.add(Conv2D(96, kernel_size=(2,2), strides=(1,1)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "# PART 2\n",
    "# FULLY CONNECTED\n",
    "# now take output from convolution nn and feed it into  fully connected nn\n",
    "model.add(Flatten()) # not lose spatial info since \"encoded\" via convolution?\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.28)) # rand set frac of inputs to 0 - improve generalizability\n",
    "\n",
    "model.add(Dense(10)) # 10 output nodes, 1 for each number\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "# Be careful with loss and metrics; accuracy =/=> categorical accuracy: \n",
    "# https://stackoverflow.com/questions/41327601/why-is-binary-crossentropy-more-accurate-than-categorical-crossentropy-for-multi\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='rmsprop',\n",
    "#               metrics=['accuracy'],)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['categorical_accuracy'],)\n",
    "\n",
    "model.fit(x_train, y_train, epochs=15, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reasoning behind some of the modeling decisions\n",
    "\n",
    "+ Square convolution window: the image itself is square. Non-square windows were tried since numbers are more rectangular in shape, but square windows still performed better.\n",
    "\n",
    "+ binary_cross_entropy & categorical accuracy: easier to treat this as a multiclass classification problem instead of a regression problem given small number of discrete digits.\n",
    "\n",
    "+ adam: this optimizer is fairly fast, requires little to no tuning, and in my limited experience, achieves results comparable to those of other optimizers (SGD, nadam, etc.).\n",
    "\n",
    "+ dropouts: dropout serves as a sort of regularization term. It seems counterintuitive to want to randomly set some activiation values to 0, but it is a little like adding randomness for which the model has to account. Overall, it does seem to help reduce overfitting.\n",
    "\n",
    "+ commented out 1st pooling layer: improved accuracy, which makes sense pooling is more of a dimensionality reduction tool rather than a feature extraction technique like convolution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very promising! Let's see how it performs on some testing data.\n",
    "\n",
    "*Note, the above model is not the first one tried; it has been tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argmax idea from: https://github.com/keras-team/keras/issues/2524\n",
    "\n",
    "def categorical_accuracy_calc(y_actual, y_predicted):\n",
    "    correct_classifications = 0 \n",
    "    \n",
    "    for actual, predicted in zip(y_actual, y_predicted):\n",
    "        correct_classifications += (np.argmax(actual) == np.argmax(predicted))\n",
    "        \n",
    "    return correct_classifications / float(len(y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN is 97.14286% accurate on validation data.\n"
     ]
    }
   ],
   "source": [
    "val_predictions = model.predict(x_val)\n",
    "acc = categorical_accuracy_calc(y_val.values, val_predictions)\n",
    "print('CNN is {0:.5f}% accurate on validation data.'.format(acc*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what digits the CNN misclassified and what the images look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA60lEQVR4nGNgGFAg0PB3Ji8uyRt//vw5H4RdLvLvyxWf/34IxCq5+O9EBu/Tfz57Y5N89D+EgYEh9XUDNsmHf+czMDAwyHBik0z5+yGQgUE4lAurpaf/fH746PkfrMYy8E74+/ff37+nESKMSLLCjgwMak0Mcs+w6mVgYFB79acQzmFCk7xVjsRBl2T4wsiIW5Lhv5wlLPhRHDTzufF/IVXG/1cnvN7CwMDAwILklfVWDIz/GRgYGC7/NNyCZp7/378NxgxJf1Ox+YJr9Z8GKftXj7HJMTDwrv5z/9UDbeySDFwN+7pxydELAABiw00mRjONdgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x1662BA7F0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:9 \t\t predicted: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAxklEQVR4nGNgoBIweWGGW3LZv34kHhOqJA/DLpySPHoMz3GaKvvvhSROySn/1uKUU3j7D7djzf7940PmozjImeH4d1waBc/9dsZpavi/16gCqP58jFMjw4Z/4TjlxD+8kGRg4ODS58IiWfnv7cqVK9//+/e7lx1djvPav3///v379/nPv3+LIEKMcEmrIwyvVy5nYHjPyzFfyvwyqs6Qf/+KocyV/yzRjJ31byPUGLGvr2UYGBhQ/fn9P4QO5TzxBKefBi8AAA7TQSVF91woAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x1662BA898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:6 \t\t predicted: 4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAxUlEQVR4nGNgGPwg84AcTrmSv3/0cclpvvh7iRuHHP/DP5+DcMipLPrzpwaHHMfcv3//6uKQnPvnz5+1CC4LkhSbTcJ/BoZnCAEmJLmpu75dQ5FEAjP/XPBv+HOcHYszMx//PW7HsP9vA6YrZ3/6+2eZKIPC268WGJJ5f76s0mdiYJjy5x6moewhhgwMDAwMf/+kYHUOAwMDQ+TfP6LIfCZkjhATIwNOSaV//3GaynD571dhnDqfMZz4jFOyleEDG25zqQYAY49BK4z30C4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x1662BA780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:1 \t\t predicted: 4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAv0lEQVR4nM3PPw4BURDH8UGI+FMoVNtJaBTCBfQSpVIjWhVqEjdYN3ADBYVG4wIrEYlOtBpbrOD7QkFl57Vimjd5n/wmMyL/UC2zdz5tNISVZ3FhxbJIYMWUyNWKJ5GzbZ/hHdo29GCb1scmGo7IMaPnCgDsVKvOAbgolBisn8YY48/CFpuCgUmzpgRdwDAKXy4S7z4Ac8hru4wBCDqaJTcALNUj3kE3paIPsMqqJh7cermvz8jndfqlbF3P/bZekzVWWW/hF8kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x1662BA8D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:2 \t\t predicted: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAvklEQVR4nGNgGOxgw/+vejA2E7qk+j8OKRz6mCI//H2Ky1Def//+bUUoRZFTPsTw//8ZOJcFWc5whwhOh9pf+vv339+/lljlPv279Ojf390YHmBgYOh8+++W7v1//3KwyPG/+ntJyu3vv78h2CUfLX/x9997QWxWJl779+/fv3++2B3LV/7v7zF1Nhxe6f77QQVFANndXIx37uDQx8Dw/m8rTrnYP/+McUru/LcNLXBQuC//4ZS88X8TTlOpCgDep0tSg2YbYwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x1662BA860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:4 \t\t predicted: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAwklEQVR4nNXOSwqBYRTG8Re5lPsGkCXIQFagRBZgRrIDazBSSnZgFQa+MjVQJIokEwMDlFv+MfC5fQ4DM8/o7fx6znmV+tsEstXq/nzelOzv5u2hp242minPPdbr5IHOtf44DVXkZGg6Jnptdps8Ld8npt9+G64NXpqGuPtA5QMWD9DxyJYZwyolW2gE27RsVg1IymapAC2bjDmgG5QttgTm8tb4AqDpFw9qADWfWIwCu7JLvmiDY0EmpUwN2p/sh1wAYOVn1/M5YewAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x1662BA780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:9 \t\t predicted: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABM0lEQVR4nNWQP0jDUBDGvxdDS8VSF4daBGdxqIiDCAqOtWM7uCgoLnGJTk4O/tkUwUEQBycHcWrANYripFgEsSBKFVHBwUGhIHxHHJKYROPm4g2Pu/u9u/d9D/hPoaLlrIP967h7xfLUvZAv6/nvJNk/1yApJPnYF2VLltt3j7uB5hDLP3h907wlhSwBADQA6LXbldLU7oJ+rJRSmlKDweA2SRq5BPDq7a4DAHQAGHMAVLcyQMbBx/nRRFsy++xPhrS8HRaAkRpN/IR2AQBwEoJDli2yau3Me3VZRAJFqRYjh1TCLxfJs7g/BIDRBp+ykU7ruJfo01Xfyhc7XXGTzk0KbyYjsEMcWTuYcUREHFmOvtNkBH5qXZ403be6kS52pwFcvmPvKkblcKVeuuj5zcPfxSfWtKJpoeSHLgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x1662BA828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:9 \t\t predicted: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABK0lEQVR4nL2QQSiDYRjHf+bbtIM4SGYOiq/lLCKkJCUp3ByUWnGRi6PLFmeUclBOWjltJ+UmhZNpO2pKWokQlk2Nfxw+32zf57z/5f0//d7ned//A9VXTXmRmLod8QcH2X76dt+MS6eSpBar9pQxwwf9vAn87sY+SR9rTdGiJlwsdC0VFqD3RSEn8yak/CLQfqeoE/ZIOgNouLE/9NcYlzJhgLGCCwak92HLXrmiDHE+egJAcymI8Xv68uvxtGXDwUroGa+PMfsMkEmvgnlf9mCdKvWwM+3BXvzyZjFxmAKgc2UAgO6U3XmsR9u2JRUxTXN3ozR2Rp9zlgtuKeKtDGkkle0CaL3UQcCxASYvlJ2HpVcdGU4GjUl95XJF7Xe4GdTuSYqFPf+xauoHZ9V7FqM7XZMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x1662BA908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:2 \t\t predicted: 4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA1ElEQVR4nMXSoQ7CQAwG4D8ENT0ss8MeGiRgt2cgYPFYwiuAn4VMzm767EZwww4L8npFMALJFQtVTb5c/6Y54L81K677WKZeRoZM7QsUbCxlPhZ2J2Bh8gkAn1wMmkv07Gzl4LFso2IqHbyqtknukYN9r9241sI+r/GUfZHBWpOpdTZyaXyk+802J7LkXslXKtSN8lRlbqEwd/hM1Pw6ROcDI04BILVn4WWblrASkAwAQJOETMt3MgB0P8fynBFOeSs8xKJmYkqEIwBAuMoPSvoKP6wHsrtZ4ztpY8gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x1662BA828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:8 \t\t predicted: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA+UlEQVR4nGNgGDaAEZUrHs/AYHHz0nJ0ZVYrXr9+/f7fv3///v3qVEXRkX3t3T8k0IlkrFhcFwMDw4uHE54xMDBYJqsyfNB5Bte4/9+/f4+C9KE8j+///pUiTH3979/fRAT3BFSSCSZwfz6mVyCS0jfRhL+fRUj++s9wHkXy6z4EO+D7vyAEz+Xrv1dIxgqxI2sT4GR4iiTJz3jnNEIykuFPO5La1/8qERyHD//akA16fVMBwfn07583iuQmBNv4+79uFiS59D8ISZN3/x6jxEnZcxsYk30NLEpgINkSxlLe8+/VHC4GrICt589sQ+xSDAwTnmTgksIAABQYaCVfJCBzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x1662BA8D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:7 \t\t predicted: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA3UlEQVR4nMXPMUsCcRjH8cdTD+ycBDmkCPdegYs0JvYKBNHNzfaGBhcJ3HsDR+I7KMIhVBoahBu9pojo0E2Co+SbiyJ0z9+xnunh+fA8/B6Rfyw7e3n9vGinVTwLlgAdFUufALxXjncza9s8nc5FRIaRra0WF4Dn6IlqQD9jiJu6hberokHlIoSXk6RBM40HGLim5VznB79g1FbExIQiH3xV9yDjTWv9tnRCxInjYU9EDry8JJR7r6sbKT8C3MdxRHQXAoTKL+4UAL6bWs5zH4C6/kW+G8y6R1qev6s1t05cJUGrFo8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x1662BA7F0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:3 \t\t predicted: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAxklEQVR4nGNgoBowevfWCJdcx7O/f5/xQDlMqHIsSuIM/3b8xK6x+e/fm2W4TL34968egseEIf8enyRu4PL3ryxOnT8YGKRx6pQ8+feeDk7Zpf/+PcApqXb3733cTir8/dkPt+yDv9vYUUVUTJEk//KhyMleew6R5ZBL+/r3LSxWWBgYGBgYLNUZSu4yMDAwykQxMHzw/oKiU+AvBPz/+/cvhmtZGn/8/fv3799/f//+XWmC4caYjO1///79/3d2Bg+G3BABAExoT+zeTp83AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x1662BA7B8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:5 \t\t predicted: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA2klEQVR4nGNgGMqA3U3NjR2HnPaVPy/+lMK5piiSJX///PnzCZs2lXt/Tz5/9vbvXyxykbdvzZzJwOD25w9MhAUu5xD43/ItDqcovfu7hIGBgYFhCUInDCjc+uPNwcDAwCB2/c8JdGPVpCVfMzAwMDAnqDHsQNfZl8PAwMDAIFb2589BDnTJHQ0MDAwMDJP+/nnpgC6n9iGCgUGbofzH3z+RGG6Vuvul7vmHZz/+fHZgw/RJyc+///+e/Pt5HqYUAwND2Y+/96sfF6KIMcJZ9myfTyg8wKpxaAAAAmVN3SvNdpQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x1662BA908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:0 \t\t predicted: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA8klEQVR4nM3PPUuCURjG8csXzMExdJDaagmkoakgsqFBCKJy9gM0tbpFc3vfoFp1cLEl5EQvix/AJQpRjCgjA/k/T8Oh4dHbNbqnC35c9zm39B/mOQiCIExLUnLSDjOhpNCq5VwP+BwO0waWAe4z5nvlV+BmybSdPrxv502bO4POpknSKQy2Zlj1Cx5m2EIDWjmfVyqliKWacLvs8+IdrQjWgEsfz7s85SUp/ovz0nfT71zLtksvkaaDvu89UilM/MfBhpRavRj39uKaxhPpGAYH05c4GNXrH7BvnOkAoLsbM/AIgDerJyWKV1wX1037y/kBSCdujlWhjSMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x1662BA780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:0 \t\t predicted: 6\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA80lEQVR4nGNgGGAQuujK5QYcclnf//39+/ccL1yACUmSie3ny9cMBlxwAUYkSf6rVzzY7YxFLu55jsXc+jeC7Bm7//1Lx2Ypp2D/i7///jaxYXWS18+/f1ebcWKVc/n39+/fNOxeCf73/2Pu9P/i2OS0H/z9180g+c8Tm+Tkv39nszAI/tuHRS70+9/bkgwMDP/OYZHc/e9fBwMDQ/n/YiyS//6+YGNgEPv+LxAuhBy2/34xMJSzMShh1bmIgaHtw99/BlgkD/9bbLzo678nWAOh4O/v7//+/vXFJsfAu+Pv338vEnixSjLwTji00AG7FB0BAGT6YEDZv4oPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x1662BA828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:8 \t\t predicted: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAw0lEQVR4nM3OsQpBcRgF8BMpN+qmsFmUgeyUUZGyKA9wH8ADKHkDm+VunsFyFeUFrIZbDEp3ugbTlXQ+NnT7/quc7f//db4O8JMU95uMEbvk+P1IxHB7/fqKYz6FnBGzSXSMWE5jZcQWsDOfhe9BT2X64DGvWzsQkidbs77IU0Skrlj1TAov0Tq+A4C1ZOAKZ7alFIe8O66wqa6ZcNGg+N+9z/0aOAdGN61YChiSnjIGgEOSfkE1ZA9k1NMNGIRro/1LXgNWR5PLf86aAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x1662BA780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:2 \t\t predicted: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAz0lEQVR4nGNgGDaAEUpzMUrOFLn7eM3pH5hq+p6++vv3799/f7dJYMhJXP377+/f/Wf//f37yBRdku3R339/J7Gc/vc19cZFEXTZ4Nv/7mtP+PPvHH/f30wMgyX///v37981EYbIf9dZMGSTvv79e0yDgUHg0z97TAerhAcxMzAwMLzDJgkDtxCSTBiSdxmMcEuyMcriljT8z4BbkoFBA5/kfyxiECDyGbdrha5xM8I56EElJ4xkKLpODWQOuuQjBobfb3G5h7nxx3vMWMEGAPKsS6N16ZUBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x1662BA908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:9 \t\t predicted: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABDklEQVR4nGNgGBRgcjp2cVFOZxGGT2/UoFwWhIy/pyXvT7WHnzm5BNE1MZX/+vPnz8tf3/78+WOOLin+58/aiAgeX9uIbzeEsUg2Q1gm2jAxuJ3//7BchLDOIKyCMX48YtDH7gWIsUYYjkRiP2ZgYODvvrNIHVPn379iDAx1///+/TuXA12Sff8fUZ6Uz3/P7boLczYCCL/9I7Xuz582XgbNP2+FMB10+M+3NlYGBsaVfxrQHPTxOIPlY/eq3wwM//8wMGP686LfYSjnFpqxS77+ieRiYGBgYBA9f4kbTZJz0ok/ryt1db1m/fgzEcOjDOLle//8+fvnz59GPkxJBgY2xYa/fxaq8mCToxcAAHTrZvPJS+vlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x1662BA898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:5 \t\t predicted: 6\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABBklEQVR4nMWPzytEURiG33ubhRp1KZmaJSspP8pCUoOFoiQ7q9nY2ShFuXbS/AOz4B+wsZqRpTVLGUpZEFKyGKXIGM+5FuPOcM6w5Fu95316v/O90r9Oa26ma+PYmOdxlwX7AAY4jC2/DnumYnUWi0QdjkiSHiIdLLprs8De/A/3DEO597uV+Pq4ux7N3OxWmicBw3n2F8jrggu3Y8jLwKfV6Bl4nnc04c9deC2TTnIVngYlTVe5DSy2VKYyK0nagmVrbWdQShclSZtSuwXv1VatqQ732BTUKnRfgV3VX+N9JwzT4SWcJu1o38kbEAGsOD1L/euSIkmPxv1V/lABA/mxJuzP5gMnxHN1xo7bKwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x1662BA908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:5 \t\t predicted: 6\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABC0lEQVR4nGNgoAlgxCLGZWj74/SDp1jVszf//ff374uNDAwsGHJetWYMG//vfYxNn/7rvyc1cbhBbc7XOlYcclyr/6/DIcXAsPzvPFEcUuz+H/664DIz7u/fv69fLZPCJrn679+/f//9/XtOHFNf4v9//45XMOid+1eOLsfX/PfnHhdmBgautX8no0oJ1zz9+3cpAwMDA4PI379WqJJT//79e52DgYGBgaHu70m0UJjzd/KFv0UMDAwMwV/+oofenH9+Gv9eSDBwrf7/EeFTJggl99/o/fb/ZhGngx477kF3q+blvw3b//39+3enDZYASHn09++/vw9rlJAF4clEKFaH4Uf9Oyz6BhEAAJ4Pannkp0c2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x1662BA780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:5 \t\t predicted: 6\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA+UlEQVR4nGNgGAzAYsGPJhxShlt//Pv3RR2rnN+rf79+/fumi03O4MW/a16H/n3wYcaUM3r+74TO4X//fq3HNFftxb8zCXf//WjUxtTHvuHfv2P//u0wxWbhsn///v37kMKETc74+79/v/bJYPUEQ++/fy+yUYUQhjAyMCyaikOSx5iBgQ+7mQwMU/9d+31HGLsc86EPDtP/nTLAKln8r4KBff2/7yrYJI/9U2Fg4F7371MeKzZJIQYGBs4J///NwibZo8DAwMDQ8P3vzVYFqCAjlHZfy/ViKgMDA2OAEQNDXwmqJIOOl6aCJgffLrdtr5VLTmH3Ej0AAFDmX8uqLDk/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x1662BA7B8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:8 \t\t predicted: 6\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABC0lEQVR4nM2QPUsDURBFT9YP0M0KRsJikdZGRVKpREEbf0E6U1iLwUKsxN7Gwi6doCD4CyJoY2UpBBJIERCEqAiBRSGg9yU2u6tkfZ2Ft7tz3p03M/B3cvbNSrI6NQIwtCcl4Ux7HZi9kBp+Aq7qKp8/N5Iuk10zNUkyUjMblYZj2Cnt+nBS5e3VNqsnbVsX8dRKx8YZgGU+361wnGNr17W+2fp2A8nlnula4STNX04QKjA7P1x4hNFFghqAaYcg9xg9mbuTOrcVt/gRRKV6nLzOwkSh4HXDEXJLZQOkALifj/9p8XKzmfYbG88RzBwCxWkg1Qfg7OApTgIw5ixU3SO3VIHTh551oX+rL7qrUB+cNpZoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x1662BA978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:1 \t\t predicted: 2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAtUlEQVR4nGNgoAcQ+/+fC8FjQpP9988fpyQzA4McTslqBoa3uKzM+PX3HjsOOfmHf++q4pAzuf33gyIuQ4/8/fujRhCHpNCxz3//HubBpVcp9crfXlySDAzqb94gPIoeQj9+CRrjlDQX/4UzFITe/n2ES45r59+/+Tjk+Lb8/XuSA1NcKZmBgX3r378LsMgxrPu7rffj37+30B3IwMDAwCDx+u/fv3/n4Qj20GV/Z1VjM3MwAQCFEj6fDBVGzwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x1662BA828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:3 \t\t predicted: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABBklEQVR4nM2Pv0qCYRjFHy1amiSowXxz6gIqwjHCQaiGLqAgsKGuoaDwEqKpOYgEKTdvIEQQalBIob9LEEVSHyT9Hmr4fIvv+143h87yHJ7DOZwj8l+RbquqfqvqQToi7gCgAIv2F7dkWU6NMSlz2HHEVp/nfdJwOKVdixiGezcx+i4yNLuQlVQ0tcjF5O45ClxPhcU9nm6Bty/gaCIkrgJ8nGQ2t1tdbDeL6Tt4nfF7w1qwUCu7cdz0aSzmWGrx54xHNDMmnZs+PlOHfPC1VF353cRVIiiuU/FJ2aM0Fw67r4iIjJc+X/Ihn4g0HgvJ5NalPuQcPQpdAO9sxNlyH/AyfSYMFj9FvHJbPLriuwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x1662BA8D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:5 \t\t predicted: 3\n"
     ]
    }
   ],
   "source": [
    "# Print the ones the CNN misclassified\n",
    "misclassified_digit_indicies = []\n",
    "for idx in range(0, len(val_predictions)):\n",
    "    if np.argmax(y_val.values[idx]) != np.argmax(val_predictions[idx]):\n",
    "        misclassified_digit_indicies.append(idx)\n",
    "\n",
    "for idx in misclassified_digit_indicies:\n",
    "    display_mnist_digit(x_val[idx])\n",
    "    print('actual:{} \\t\\t predicted: {}'.format(np.argmax(y_val.values[idx]), np.argmax(val_predictions[idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While searching around about how to reduce overfitting in a convolutional neural network and how to squeeze out more performance, I stumbled upon this wordpress: https://shaoanlu.wordpress.com/2017/05/29/sgd-all-which-one-is-the-best-optimizer-dogs-vs-cats-toy-experiment/. According to this person's summary of a journal article, adam optimizers do not build good generalizable models; they overfit the training data. The recommended approach is to, instead, do a combination of adam and stochastic gradient descenet (SGD).\n",
    "\n",
    "Fortunately, Keras makes this rather simple according to: https://stackoverflow.com/questions/47626254/changing-optimizer-in-keras-during-training?rq=1. The model can be recompiled with a different optimizer without losing its weights that it already learned from training. The recompiled model can be fitted, and the new optimizer will be used, picking up where the previous compilation left off. I am interested to see if this will actually meaningfully improve performance, so let's try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "39900/39900 [==============================] - 490s 12ms/step - loss: 0.0019 - categorical_accuracy: 0.9984\n",
      "Epoch 2/3\n",
      "39900/39900 [==============================] - 483s 12ms/step - loss: 0.0017 - categorical_accuracy: 0.9985\n",
      "Epoch 3/3\n",
      "39900/39900 [==============================] - 480s 12ms/step - loss: 0.0017 - categorical_accuracy: 0.9983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x159233ef0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "model_fine_tuned = model\n",
    "\n",
    "model_fine_tuned.compile(loss='binary_crossentropy',\n",
    "                         optimizer=SGD(lr=0.03, momentum=0.2),\n",
    "                         metrics=['categorical_accuracy'],)\n",
    "\n",
    "model_fine_tuned.fit(x_train, y_train, epochs=3, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cnn_3_layers_submission.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN is 98.66667% accurate on validation data.\n"
     ]
    }
   ],
   "source": [
    "val_predictions = model_fine_tuned.predict(x_val)\n",
    "acc = categorical_accuracy_calc(y_val.values, val_predictions)\n",
    "print('CNN is {0:.5f}% accurate on validation data.'.format(acc*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! That's actually a meaningful improvement, at least for the Kaggle public leaderboards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to csv\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "test = pandas.read_csv('test.csv')\n",
    "\n",
    "result = pandas.DataFrame({'ImageId': [img_id for img_id in range(1, test.shape[0]+1)]})\n",
    "\n",
    "result['Label'] = model_fine_tuned.predict_classes(test.values.reshape( test.shape[0], 28, 28, 1) )\n",
    "\n",
    "print('writing to csv')\n",
    "# no label column in test, so can just reshape values\n",
    "result.to_csv('mnist_cnn_3_layers_25_dropout.csv', index=False, sep=',')\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, there are other improvements that could be made to the model. More hidden layers could be added (6 hidden convolution layers seems to be popular), the digits could be deskewed (de-slanted, really), and the image could be elastic-distored (shrinking or stretching the image at random points) (see https://en.wikipedia.org/wiki/MNIST_database#Performance). All of those things, however, are essentially just copy and pasting other peoples' solutions from research papers and blogs. Submitting solutions that use those things feels dishonest, so this is where I will leave off. \n",
    "\n",
    "The final model scored was **98.985%** accurate on the Kaggle leaderboard. That puts the model at 1056th place out of 2658 teams (**top 40%**). For reference, humans on average supposedly correctly identify 97.5% to 98% of digits, so this model exceeds that level of accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
