{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predicting Titanic Survivors\n",
    "This file uses uses feature engineering, missing entry imputation, and machine learning to attempt to predict who did and who did not survive the \n",
    "Titanic. The data used here comes from Kaggle.com (specifically here: https://www.kaggle.com/c/titanic/data). As of Jun. 18, 2018, placing in the top 1,000 (out of 11,356) requires accuracy of: 80.382%. My best model submitted placed at 2,532nd place with Kaggle accuracy of 79.425%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules and the data\n",
    "import pandas\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "df_train = pandas.read_csv('train.csv')\n",
    "df_test = pandas.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch',\n",
      "       'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
      "      dtype='object')\n",
      "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
      "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_test.columns)\n",
    "print(df_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and testing data will be analyzed together in order to make sure that feature engineering covers everything. The two can be unseparated later once the feature engineering and imputation steps are complete. The 'Survived' column entries for the test set are NaN, but for the tranining set, they are all 1's and 0's. Therefore, as long as the rows don't get mixed up, a query of 'Survived == NaN' should do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>1309</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>Peter, Master. Michael J</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2668</td>\n",
       "      <td>22.3583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      PassengerId  Survived  Pclass                      Name   Sex  Age  \\\n",
       "1308         1309       NaN       3  Peter, Master. Michael J  male  NaN   \n",
       "\n",
       "      SibSp  Parch Ticket     Fare Cabin Embarked  \n",
       "1308      1      1   2668  22.3583   NaN        C  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined = pandas.concat([df_train, df_test], sort=False).reset_index(drop=True)\n",
    "df_combined.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to get a sense of the data, print the 3 most common values (including NaN's) in each column and the number of missing entries in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1309    1\n",
      "449     1\n",
      "431     1\n",
      "Name: PassengerId, dtype: int64\n",
      "There are 0 empty entries (0%) in PassengerId\n",
      "\n",
      " 0.0    549\n",
      "NaN     418\n",
      " 1.0    342\n",
      "Name: Survived, dtype: int64\n",
      "There are 418 empty entries (31%) in Survived\n",
      "\n",
      "3    709\n",
      "1    323\n",
      "2    277\n",
      "Name: Pclass, dtype: int64\n",
      "There are 0 empty entries (0%) in Pclass\n",
      "\n",
      "Connolly, Miss. Kate           2\n",
      "Kelly, Mr. James               2\n",
      "Andersson, Mr. Johan Samuel    1\n",
      "Name: Name, dtype: int64\n",
      "There are 0 empty entries (0%) in Name\n",
      "\n",
      "male      843\n",
      "female    466\n",
      "Name: Sex, dtype: int64\n",
      "There are 0 empty entries (0%) in Sex\n",
      "\n",
      "NaN      263\n",
      " 24.0     47\n",
      " 22.0     43\n",
      "Name: Age, dtype: int64\n",
      "There are 263 empty entries (20%) in Age\n",
      "\n",
      "0    891\n",
      "1    319\n",
      "2     42\n",
      "Name: SibSp, dtype: int64\n",
      "There are 0 empty entries (0%) in SibSp\n",
      "\n",
      "0    1002\n",
      "1     170\n",
      "2     113\n",
      "Name: Parch, dtype: int64\n",
      "There are 0 empty entries (0%) in Parch\n",
      "\n",
      "CA. 2343    11\n",
      "1601         8\n",
      "CA 2144      8\n",
      "Name: Ticket, dtype: int64\n",
      "There are 0 empty entries (0%) in Ticket\n",
      "\n",
      "8.05     60\n",
      "13.00    59\n",
      "7.75     55\n",
      "Name: Fare, dtype: int64\n",
      "There are 1 empty entries (0%) in Fare\n",
      "\n",
      "NaN            1014\n",
      "C23 C25 C27       6\n",
      "G6                5\n",
      "Name: Cabin, dtype: int64\n",
      "There are 1014 empty entries (77%) in Cabin\n",
      "\n",
      "S    914\n",
      "C    270\n",
      "Q    123\n",
      "Name: Embarked, dtype: int64\n",
      "There are 2 empty entries (0%) in Embarked\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in df_combined.columns:\n",
    "    # print top 3 most frequent values and their counts\n",
    "    top_val_counts = df_combined[col].value_counts(dropna=False).nlargest(3)\n",
    "    print('{}'.format(top_val_counts))\n",
    "    \n",
    "    # print num empties just in case it doesn't show up in the top 7\n",
    "    num_empties = df_combined[col].isnull().sum()\n",
    "    percent_empties = int(100 * num_empties / df_combined.shape[0])\n",
    "    print('There are {} empty entries ({}%) in {}\\n'.format(num_empties, percent_empties, col))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "+These are ready to go: PassengerId, Survived, Pclass, SibSp, Parch, Fare (except the 1 missing entry)\n",
    "\n",
    "+These just need encoded: Sex, Embarked\n",
    "\n",
    "+These need a lot of work:\n",
    "\n",
    "Age - need to impute missing ages \n",
    "\n",
    "Name - extract titles (Mr, Master, Mrs, etc.)\n",
    "\n",
    "Ticket - separate ticket prefixes (if present) and ticket numbers\n",
    "\n",
    "Cabin - mostly empty; split empty/not-empty?\n",
    "\n",
    "\n",
    "That covers all 12 of the original columns\n",
    "\n",
    "A great place to start seems to be 'Fare' since it just needs 1 value imputed. Given the small scale, the mean fare is imputed for the missing fare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use mean fare to impute the 1 missing fare\n",
    "nan_fare_idx = df_combined.query(\"Fare == 'NaN'\").index[0]\n",
    "df_combined.at[nan_fare_idx, 'Fare'] = df_combined['Fare'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next easiest things to do are to encode Embarked and Sex. Each is dummy encoded. Embarked is encoded via pandas' get_dummies since it has more than 2 unique values while Sex is converted manually since it only has 2 unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy encode Embarked\n",
    "cols_to_dummy_encoded = ['Embarked']\n",
    "df_combined = df_combined.join(pandas.get_dummies(data=df_combined[cols_to_dummy_encoded],\n",
    "                                            dummy_na=True))\n",
    "# Done with original Embarked col., so go ahead and drop it\n",
    "df_combined = df_combined.drop(cols_to_dummy_encoded, axis=1)\n",
    "\n",
    "\n",
    "# Dummy encode Sex\n",
    "df_combined['Sex_m'] = (df_combined['Sex'] == 'male') * 1  \n",
    "df_combined = df_combined.drop('Sex', axis=1)  # Done with original Sex col.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId                   1044\n",
       "Survived                       NaN\n",
       "Pclass                           3\n",
       "Name            Storey, Mr. Thomas\n",
       "Age                           60.5\n",
       "SibSp                            0\n",
       "Parch                            0\n",
       "Ticket                        3701\n",
       "Fare                       33.2955\n",
       "Cabin                          NaN\n",
       "Embarked_C                       0\n",
       "Embarked_Q                       0\n",
       "Embarked_S                       1\n",
       "Embarked_nan                     0\n",
       "Sex_m                            1\n",
       "Name: 1043, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just to check that everything so far has been successful\n",
    "df_combined.iloc[nan_fare_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age still needs to be imputed, but that should be done at the end when the imputer can work with all columns. Let's extract ticket prefix and ticket number. Tickets have 0, 1, or 2 prefixes, and ticket numbers may or may not be present. Given the variable format of entries in the Ticket column, splitting on whitespace seems like a good first step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50 unique ticket prefixes\n"
     ]
    }
   ],
   "source": [
    "list_of_tickets = []\n",
    "prefixes = []\n",
    "for ticket_string in df_combined['Ticket'].values:\n",
    "    split_on_whitespace = str.split(ticket_string)\n",
    "    \n",
    "    # if only 1 part to the ticket id\n",
    "    if len(split_on_whitespace) == 1:\n",
    "        try: # try to cast string to an int\n",
    "            list_of_tickets.append( [int(split_on_whitespace[0])] )\n",
    "        except ValueError:\n",
    "            list_of_tickets.append( [ split_on_whitespace[0] ] )\n",
    "            prefixes.append( split_on_whitespace[0] )\n",
    "       \n",
    "    # else assumed to have at least 2 parts; only care about 1st and last parts\n",
    "    else:\n",
    "        list_of_tickets.append( [split_on_whitespace[0], int(split_on_whitespace[-1])] )\n",
    "        prefixes.append( split_on_whitespace[0] )\n",
    "\n",
    "print('There are {} unique ticket prefixes'.format(len(np.unique(prefixes))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50 (+ 1 for those with no prefix) unique ticket prefixes is far too many. Dummy encoding ticket prefixes will create 50 very sparse (has a lot of 0's) columns. Let's see if there is anyway to reduce this number by combining some prefixes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique ticket prefixes are:\n",
      "['A.' 'A./5.' 'A.5.' 'A/4' 'A/4.' 'A/5' 'A/5.' 'A/S' 'A4.' 'AQ/3.' 'AQ/4'\n",
      " 'C' 'C.A.' 'C.A./SOTON' 'CA' 'CA.' 'F.C.' 'F.C.C.' 'Fa' 'LINE' 'LP'\n",
      " 'P/PP' 'PC' 'PP' 'S.C./A.4.' 'S.C./PARIS' 'S.O./P.P.' 'S.O.C.' 'S.O.P.'\n",
      " 'S.P.' 'S.W./PP' 'SC' 'SC/A.3' 'SC/A4' 'SC/AH' 'SC/PARIS' 'SC/Paris'\n",
      " 'SCO/W' 'SO/C' 'SOTON/O.Q.' 'SOTON/O2' 'SOTON/OQ' 'STON/O' 'STON/O2.'\n",
      " 'STON/OQ.' 'SW/PP' 'W./C.' 'W.E.P.' 'W/C' 'WE/P']\n"
     ]
    }
   ],
   "source": [
    "print('The unique ticket prefixes are:')\n",
    "print('{}'.format(np.unique(prefixes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just examining the list visually and based on this, https://www.encyclopedia-titanica.org/community/threads/ticket-numbering-system.20348/page-2, it seems that there are 10 (including the absent 'None' prefix). \n",
    "\n",
    "### 1 - 1st letter is A\n",
    "A.\n",
    "A./5.\n",
    "A.5.\n",
    "A/4\n",
    "A/4.\n",
    "A/5\n",
    "A/5.\n",
    "A/S\n",
    "A4.\n",
    "AQ/3.\n",
    "AQ/4\n",
    "\n",
    "### 2 - 1st letter is C\n",
    "C\n",
    "C.A.\n",
    "C.A./SOTON\n",
    "CA\n",
    "CA.\n",
    "\n",
    "### 3 - 1st letter is F\n",
    "F.C.\n",
    "F.C.C.\n",
    "Fa\n",
    "\n",
    "### 4 - 1st letter is L\n",
    "LINE\n",
    "LP\n",
    "\n",
    "### 5 - 1st letter is P\n",
    "P/PP\n",
    "PC\n",
    "PP\n",
    "\n",
    "### 6 - First 2 letters are S and C\n",
    "S.C./A.4.\n",
    "S.C./PARIS\n",
    "SC\n",
    "SC/A.3\n",
    "SC/A4\n",
    "SC/AH\n",
    "SC/PARIS\n",
    "SC/Paris\n",
    "SCO/W\n",
    "\n",
    "### 7 - 1st 2 are NOT S and C\n",
    "S.O./P.P.\n",
    "S.O.C.\n",
    "S.O.P.\n",
    "S.P.\n",
    "S.W./PP\n",
    "SO/C\n",
    "SW/PP\n",
    "\n",
    "\n",
    "### 8 - SOTON or STON\n",
    "SOTON/O.Q.\n",
    "SOTON/O2\n",
    "SOTON/OQ\n",
    "STON/O\n",
    "STON/O2.\n",
    "STON/OQ.\n",
    "\n",
    "### 9 - 1st letter is W\n",
    "W./C.\n",
    "W.E.P.\n",
    "W/C\n",
    "WE/P\n",
    "\n",
    "### 10 - No prefix\n",
    "None\n",
    "\n",
    "Going from 51 prefixes to 10 is a great improvement, so let's go with these 10 groupings. \n",
    "\n",
    "\n",
    "The other thing to worry about is tickets that do not have a number. In such cases, a -1 will be assigned for the ticket number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row_num, parsed_ticket_list in enumerate(list_of_tickets):\n",
    "    if len(parsed_ticket_list) == 1:\n",
    "        single_entry = parsed_ticket_list[0]\n",
    "        try:\n",
    "            # if the 1 entry is just a number\n",
    "            int(single_entry) \n",
    "            # append to front of list\n",
    "            list_of_tickets[row_num] = ['None'] + list_of_tickets[row_num] \n",
    "        except ValueError: # else, must be text\n",
    "            list_of_tickets[row_num] = list_of_tickets[row_num] + [-1]\n",
    "            \n",
    "    else: # must be 2 entries in this row (representing the parsed ticket)\n",
    "        first_letter = parsed_ticket_list[0][0]\n",
    "        if first_letter is 'S':\n",
    "            prefix = parsed_ticket_list[0].replace('.', '')\n",
    "            if prefix[0:2] is 'SC':\n",
    "                list_of_tickets[row_num][0] = 'SC'\n",
    "            elif prefix[0:4] == 'SOT' or prefix[0:4] == 'STO':\n",
    "                list_of_tickets[row_num][0] = 'SOT'\n",
    "            else:\n",
    "                list_of_tickets[row_num][0] = 'other_S'\n",
    "        else: # first_letter is in ['A', 'C', 'F', 'L', 'P', 'W']:\n",
    "            list_of_tickets[row_num][0] = first_letter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this point, there is nothing left to do to with the tickets, so the ticket prefix and ticket number can be added to the dataframe now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for comparing the length of the extracted columns\n",
    "# and the original data frame. If they are the same length, \n",
    "# then it should be o.k. add the extracted columns. \n",
    "def safe_to_add_col(df, col_list, col_name):\n",
    "    same_length = len(col_list) == len(df)\n",
    "    print('List of {} and combined data frame are same length (safe to add '\n",
    "          'to the dataframe)? {}'.format(col_name, same_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of tickets and combined data frame are same length (safe to add to the dataframe)? True\n"
     ]
    }
   ],
   "source": [
    "safe_to_add_col(df_combined, list_of_tickets, 'tickets')\n",
    "\n",
    "df_combined['Ticket Prefix'] = [row[0] for row in list_of_tickets]\n",
    "df_combined['Ticket Number'] = [row[1] for row in list_of_tickets]\n",
    "\n",
    "# no longer need original column since it was split and parsed\n",
    "df_combined = df_combined.drop('Ticket', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now moving on to Cabin, let's see if just throwing pandas.get_dummies at the column would be a good idea. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 187 unique Cabins\n"
     ]
    }
   ],
   "source": [
    "val_counts = df_combined['Cabin'].value_counts(dropna=False)\n",
    "print('There are {} unique Cabins'.format(val_counts.shape[0]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It is safe to say that outright dummy encoding Cabin, which would create 187 additional columns, is not a great idea. With there being so many missing entries (the NaNs), there are limited options here. One option would be to only extract the cabin prefix (A's, B's, C's, etc.) from Cabin and dummy encode those. Those columns, however, would still be very sparse, and it is doubtful that the Cabin prefixes would provide information that Class (which does not have missing entries) does not already provide. Therefore, a column that tracks whether or not Cabin is empty will replace Cabin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['Cabin_None'] = (df_combined['Cabin'].isnull()) * 1\n",
    "#df_combined.head(5) # checked that encoding went o.k.\n",
    "df_combined = df_combined.drop('Cabin', axis=1) # done w/ this col.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that remains for the data preparation part is extracting information from Name, and imputing missing ages. Therefore, let's take a look at some of the names to see what we're dealing with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbing, Mr. Anthony\n",
      "Appleton, Mrs. Edward Dale (Charlotte Lamson)\n",
      "Bazzani, Miss. Albina\n",
      "Bradley, Mr. George (\"George Arthur Brayton\")\n",
      "Carlsson, Mr. August Sigfrid\n",
      "Collander, Mr. Erik Gustaf\n",
      "Daniels, Miss. Sarah\n",
      "Drew, Mr. James Vivian\n",
      "Ford, Mrs. Edward (Margaret Ann Watson)\n",
      "Goldschmidt, Mr. George B\n",
      "Harper, Mr. Henry Sleeper\n",
      "Hogeboom, Mrs. John C (Anna Andrews)\n",
      "Johansson, Mr. Nils\n",
      "Kilgannon, Mr. Thomas J\n",
      "Lennon, Mr. Denis\n",
      "Mallet, Mr. Albert\n",
      "Milling, Mr. Jacob Christian\n",
      "Natsch, Mr. Charles H\n",
      "Olsson, Mr. Oscar Wilhelm\n",
      "Perkin, Mr. John Henry\n",
      "Richard, Mr. Emile\n",
      "Sage, Master. William Henry\n",
      "Sivola, Mr. Antti Wilhelm\n",
      "Strandberg, Miss. Ida Sofia\n",
      "Troutt, Miss. Edwina Celia \"Winnie\"\n",
      "West, Mr. Edwy Arthur\n",
      "de Messemaeker, Mrs. Guillaume Joseph (Emma)\n"
     ]
    }
   ],
   "source": [
    "# Checkout what the names look like again - sorted order really helps btw\n",
    "n = 53\n",
    "for count, name in enumerate(sorted(df_combined['Name'].values)):\n",
    "   if count % 50 == 0: # print every  nth name\n",
    "       print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things to notice from the above: \n",
    "\n",
    "+ Last name is always first and that it is separated by a comma from the rest of a person's name. \n",
    "\n",
    "+ Titles have a '.' at the end of them\n",
    "\n",
    "+ Those with nicknames have their nicknames in quotes (\"...\")\n",
    "\n",
    "Based on these differences, it should be fairly easy to extract last name and title. Last name will be extracted and stored in a dictionary where the key is the last name, and a list of passenger ids with that last name is the value. Presently, I don't know how to use last names, but hopefully I will think of something later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of lnames and tickets and combined data frame are same length (safe to add to the dataframe)? True\n"
     ]
    }
   ],
   "source": [
    "list_of_lnames_titles = []\n",
    "family_dict = {}  # key is lname; value is list of passenger ids w/ lname\n",
    "for row in df_combined[['PassengerId', 'Name']].values:\n",
    "    passenger_id = row[0]\n",
    "    name =  row[1]\n",
    "    \n",
    "    # extract last name and title\n",
    "    comma_separated = name.split(',')\n",
    "    lname = comma_separated[0].strip(' ')  # strip whitespace for uniformity\n",
    "    title = comma_separated[1].split('.')[0].strip(' ')\n",
    "    \n",
    "    list_of_lnames_titles.append([lname, title])\n",
    "    \n",
    "    # now append to dictionary\n",
    "    if lname in family_dict.keys(): # if family name already in dict\n",
    "        family_dict[lname].append(passenger_id)\n",
    "    else:  # else not already in dict, so set it up\n",
    "        family_dict.update( {lname: [passenger_id]} )\n",
    "        \n",
    "safe_to_add_col(df_combined, list_of_lnames_titles, 'lnames and tickets')\n",
    "\n",
    "df_combined['Last Name'] = [lname_title[0] for lname_title in list_of_lnames_titles]\n",
    "df_combined['Title'] = [lname_title[1] for lname_title in list_of_lnames_titles]\n",
    "   \n",
    "# done w/ name, so go ahead and drop the column (columns are on axis 1, not 0)\n",
    "df_combined = df_combined.drop('Name', axis=1)\n",
    "df_combined = df_combined.drop('Last Name', axis=1) # as I said, I don't know how to use it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally get to some machine learning. Machine learning will be used to impute the missing ages in the data set. Various algorithms will be tried, but as a baseline, let's get a mean squared error (MSE, which measures bias and variance) for imputing the mean for all missing ages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for telling the mean squared error of an imputer method\n",
    "def age_mse_results(y_actual, y_predicted, imputer_uses_string):\n",
    "    mse = mean_squared_error(y_actual, y_predicted)\n",
    "    \n",
    "    print('MSE for Age imputation that uses {0}: {1:.2f}'.format(imputer_uses_string, mse))\n",
    "    print('\\t ==> age predictions are off by {0:.1f} years on average'.format(math.sqrt(mse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy encoding since python's machine learning algorithms only take numbers\n",
    "cols_to_dummy_encode = ['Ticket Prefix', 'Title']\n",
    "df_combined = df_combined.join( pandas.get_dummies(df_combined[cols_to_dummy_encode]) )\n",
    "df_combined = df_combined.drop(cols_to_dummy_encode, axis=1)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ages_present = df_combined.dropna(subset=['Age']).reset_index(drop=True)\n",
    "\n",
    "# Split into training (for fitting) and testing data (for testing general-\n",
    "# izability of the imputer)\n",
    "x_train_age, x_test_age, y_train_age, y_test_age =\\\n",
    "train_test_split(ages_present.drop(['Age', 'Survived'], axis=1),\n",
    "                 ages_present['Age'],\n",
    "                 test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Age imputation that uses mean: 204.30\n",
      "\t ==> age predictions are off by 14.3 years on average\n"
     ]
    }
   ],
   "source": [
    "# Just imputing the mean age for all missing ages\n",
    "age_mse_results(y_test_age, [y_train_age.mean()] * len(y_test_age), 'mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good next step would be to try linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Age imputation that uses lin. regression: 120.94\n",
      "\t ==> age predictions are off by 11.0 years on average\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/base.py:509: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  linalg.lstsq(X, y)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(x_train_age, y_train_age)\n",
    "age_mse_results(y_test_age, lr.predict(x_test_age), 'lin. regression')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decent improvement! Let's try some other algorithms just to see if we can do any better. Before doing that, however, the age data will be standard scaled. This can help algorithms converge faster by scaling everything down have a mean of 0 with variance of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Age imputation that uses decision tree regressor: 152.00\n",
      "\t ==> age predictions are off by 12.3 years on average\n",
      "MSE for Age imputation that uses k neighbors regressor: 137.56\n",
      "\t ==> age predictions are off by 11.7 years on average\n",
      "MSE for Age imputation that uses random forest regressor: 118.23\n",
      "\t ==> age predictions are off by 10.9 years on average\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "sc = StandardScaler().fit(x_train_age, y_train_age)\n",
    "x_train_age = sc.transform(x_train_age)\n",
    "x_test_age = sc.transform(x_test_age)\n",
    "\n",
    "# How about a regression tree?\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# set some of the mins in order to avoid overfitting too much\n",
    "dtr = DecisionTreeRegressor(min_samples_split=int( len(x_train_age) * 0.01 ), \n",
    "                            min_samples_leaf=int( len(x_train_age) * 0.01 ),\n",
    "                            min_impurity_decrease=0.003)\n",
    "dtr.fit(x_train_age, y_train_age)\n",
    "\n",
    "# Let's step it up: K neighbors regressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knr = KNeighborsRegressor(n_neighbors=7, algorithm='brute')\n",
    "knr.fit(x_train_age, y_train_age)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfr = RandomForestRegressor(max_depth=15, n_estimators=1000)\n",
    "rfr.fit(x_train_age, y_train_age)\n",
    "\n",
    "age_mse_results(y_test_age, dtr.predict(x_test_age), 'decision tree regressor')\n",
    "age_mse_results(y_test_age, knr.predict(x_test_age), 'k neighbors regressor')   \n",
    "age_mse_results(y_test_age, rfr.predict(x_test_age), 'random forest regressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the random forest regressor generally has the lowest mean squared error, so that is the one that will be used to impute the missing ages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all rows w/ missing ages\n",
    "nan_ages = df_combined[df_combined.Age.isnull() == True]\n",
    "imputed_ages = rfr.predict(sc.transform(nan_ages.drop(['Age', 'Survived'], axis=1)))\n",
    "\n",
    "for idx, imputed_age in zip(nan_ages.index, imputed_ages):\n",
    "    df_combined.at[idx, 'Age'] = imputed_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case these age imputation were terrible, a column will be added telling if the age was estimated. 'Age Was Estimated' will also include those ages that, by the data dictionary's explanation, were present but estimated. Such ages are greater than 1 and have the form xx.5 (e.g., 23.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Age  Age Was Estimated  Cabin_None  Embarked_C  Embarked_Q  \\\n",
      "1307  30.457594                  1           1           0           0   \n",
      "1308   8.084847                  1           1           1           0   \n",
      "\n",
      "      Embarked_S  Embarked_nan     Fare  Parch  PassengerId  \\\n",
      "1307           1             0   8.0500      0         1308   \n",
      "1308           0             0  22.3583      1         1309   \n",
      "\n",
      "             ...          Title_Master  Title_Miss  Title_Mlle  Title_Mme  \\\n",
      "1307         ...                     0           0           0          0   \n",
      "1308         ...                     1           0           0          0   \n",
      "\n",
      "      Title_Mr  Title_Mrs  Title_Ms  Title_Rev  Title_Sir  Title_the Countess  \n",
      "1307         1          0         0          0          0                   0  \n",
      "1308         0          0         0          0          0                   0  \n",
      "\n",
      "[2 rows x 42 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Age Was Estimated</th>\n",
       "      <th>Cabin_None</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Embarked_nan</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Parch</th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>...</th>\n",
       "      <th>Title_Master</th>\n",
       "      <th>Title_Miss</th>\n",
       "      <th>Title_Mlle</th>\n",
       "      <th>Title_Mme</th>\n",
       "      <th>Title_Mr</th>\n",
       "      <th>Title_Mrs</th>\n",
       "      <th>Title_Ms</th>\n",
       "      <th>Title_Rev</th>\n",
       "      <th>Title_Sir</th>\n",
       "      <th>Title_the Countess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  Age Was Estimated  Cabin_None  Embarked_C  Embarked_Q  Embarked_S  \\\n",
       "0  22.0                  0           1           0           0           1   \n",
       "\n",
       "   Embarked_nan  Fare  Parch  PassengerId         ...          Title_Master  \\\n",
       "0             0  7.25      0            1         ...                     0   \n",
       "\n",
       "   Title_Miss  Title_Mlle  Title_Mme  Title_Mr  Title_Mrs  Title_Ms  \\\n",
       "0           0           0          0         1          0         0   \n",
       "\n",
       "   Title_Rev  Title_Sir  Title_the Countess  \n",
       "0          0          0                   0  \n",
       "\n",
       "[1 rows x 42 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined['Age Was Estimated'] = 0\n",
    "\n",
    "estimated_age_indecies = df_combined.query(\"Age % 1.0 != 0.0 and Age >= 1.0\").index\n",
    "for idx in estimated_age_indecies:\n",
    "    df_combined.at[idx, 'Age Was Estimated'] = 1\n",
    "    \n",
    "# Just put cols in alphabetical order just to have a known ordering to cols\n",
    "df_combined = df_combined[ sorted(df_combined.columns) ]\n",
    "\n",
    "# Just check that everything went OK and that all columns are numbers\n",
    "print(df_combined.tail(2))\n",
    "df_combined.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, all data prepartaion (feature extraction, imputation, and encoding) has been done. Therefore, we are ready to move on to building a model to classify Titanic survivors. \n",
    "\n",
    "Before building the model, however, all columns (excpet Survived and PassengerId) will be StandardScaled. This is done to improve performane on neural networks (faster convergence) and k neighbors (distances between points makes more sense since no columns have different scales)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go column by column and standard scale each column\n",
    "cols_to_std_scale = ['Age', 'Fare']\n",
    "for col in cols_to_std_scale:\n",
    "    df_combined[col] = StandardScaler().fit_transform(df_combined[col].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that will be used to train, tune, and test the model (train.csv) has to be split from the data that will be submitted to Kaggle (test.csv).  The two can be split from df_combined by querying the Survived column for NaNs/not NaNs or by querying PassengerId for values from 1 to 891 and 892 to 1309 respectively.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_combined[df_combined.Survived.isnull() == False].reset_index(drop=True)\n",
    "\n",
    "df_submission = df_combined[df_combined.Survived.isnull() == True].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was alluded to already, df_train will be split into a few parts. First, it will be split into training and testing data. Then, testing data will be split into training data (again) and validation data. Training data will be used to fit/train the model. Validation data will be used to tune the model parameters. Testing data will be used at the very very end, when the model tuning process is complete. The idea behind tuning on validation  and testing on separtate testing data for 2 reasons.\n",
    "1. Ensure that the model is not just memorizing the training data. This can be checked by comparing the accuracies of predictions on the training set and predictions on the validation set. If the training set's predictions are more accurate than the validation set's, then the model is overfitting (memorizing, really, the training data). If the validation set's predictions are more accurate, then the model is underfitting (not memorizing, really, enough of the training data). \n",
    "\n",
    "2. Get a sense of how the model performs on unseen data (testing, not validation data))\n",
    ".\n",
    "\n",
    "Fixed cutoffs are (0 to 623 and 624 to 891) are used for the initial train/test split in order to really be sure that the model has never seen the testing data. Fixed cutoffs are not used for the next train/validation split because validation is just used to make sure that the model is not memorizing the parameters on a specific run. Now, it is possible to overfit the training and validation set, but that is much harder to do. Also, if the training and validation sets are being overfitted, then the predictions on the separate testing data should be MUCH less accurate.\n",
    "\n",
    "Also, PassengerId will be kept to be able to check that the query was correct and for the format that Kaggle requires for its submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_of_training_data = 0.7\n",
    "train_test_cutoff = int(df_train.shape[0] * frac_of_training_data)\n",
    "\n",
    "# train/test split\n",
    "df_test = df_train.iloc[train_test_cutoff+1:]\n",
    "df_train = df_train.iloc[0:train_test_cutoff]\n",
    "\n",
    "# now train/val split\n",
    "x_train, x_val, y_train, y_val =\\\n",
    "train_test_split(df_train.drop('Survived', axis=1),\n",
    "                 df_train['Survived'],\n",
    "                 test_size=0.3,) # test_size really is validation size here\n",
    "train_val_cutoff = int(df_train.shape[0] * frac_of_training_data)\n",
    "\n",
    "x_test = df_test.drop('Survived', axis=1)\n",
    "y_test = df_test['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few different algorithms will be tried. Again, to get a good baseline, logistic regression will be used first, followed by a random forest classifier, k neighbors classifier, and a multilayer perceptron classifier (neural network). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to print the accuracy score of a Titanic survivor classification model\n",
    "def classify_acc_results(y_actual, y_predicted, model_string):\n",
    "    acc = accuracy_score(y_actual, y_predicted)\n",
    "    print('ACC {0}: {1:.2f}%'.format(model_string, 100.0 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC train logistic regression: 82.57%\n",
      "ACC val logistic regression: 82.89%\n",
      "ACC train random forest classifier: 86.24%\n",
      "ACC val random forest classifier: 83.42%\n",
      "ACC train k neighbors classifier: 63.76%\n",
      "ACC val k neighbors classifier: 65.24%\n",
      "ACC train mlp classifier: 42.89%\n",
      "ACC val mlp classifier: 45.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC train lda mlp: 77.29%\n",
      "ACC val lda mlp: 77.54%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(tol=1e-8, max_iter=10000, C=1.1)\n",
    "lr.fit(x_train, y_train)\n",
    "\n",
    "classify_acc_results(y_train, lr.predict(x_train), 'train logistic regression')\n",
    "classify_acc_results(y_val, lr.predict(x_val), 'val logistic regression')\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=1000,\n",
    "                             max_depth=5)\n",
    "rfc.fit(x_train, y_train)\n",
    "\n",
    "classify_acc_results(y_train, rfc.predict(x_train), 'train random forest classifier')\n",
    "classify_acc_results(y_val, rfc.predict(x_val), 'val random forest classifier')\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knc = KNeighborsClassifier(n_neighbors=28)\n",
    "knc.fit(x_train, y_train)\n",
    "\n",
    "classify_acc_results(y_train, knc.predict(x_train), 'train k neighbors classifier')\n",
    "classify_acc_results(y_val, knc.predict(x_val), 'val k neighbors classifier')\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlpc = MLPClassifier(hidden_layer_sizes=(40),\n",
    "                     alpha=.0005,\n",
    "                     learning_rate='constant',\n",
    "                     solver='adam',\n",
    "                     activation='relu',\n",
    "                     max_iter=1000,\n",
    "                     tol=1e-9)\n",
    "mlpc.fit(x_train, y_train)\n",
    "\n",
    "classify_acc_results(y_train, mlpc.predict(x_train), 'train mlp classifier')\n",
    "classify_acc_results(y_val, mlpc.predict(x_val), 'val mlp classifier')\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "x_train = lda.fit_transform(x_train, y_train)\n",
    "x_val = lda.transform(x_val)\n",
    "\n",
    "mlp_lda = MLPClassifier(hidden_layer_sizes=[200,], alpha=26)\n",
    "mlp_lda.fit(x_train, y_train)\n",
    "\n",
    "classify_acc_results(y_train, mlp_lda.predict(x_train), 'train lda mlp')\n",
    "classify_acc_results(y_val, mlp_lda.predict(x_val), 'val lda mlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite surprisingly, it seems that logistic regression did the best of all the bunch. Let's choose the logistic regression model as the one to go with. (original comment; see updates at bottom)\n",
    "\n",
    "Now that the final model has been selected, let's see how well it performs on the unseen testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC testing mlp w/ LDA (FINAL MODEL): 82.02%\n"
     ]
    }
   ],
   "source": [
    "classify_acc_results(y_test, mlp_lda.predict(lda.transform(x_test)), 'testing mlp w/ LDA (FINAL MODEL)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This level of accuracy is what we can expect for the submissions to Kaggle. Let's get our predictions for the test.csv data (found in df_submission if the query was correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_submission = df_submission.drop('Survived', axis=1)\n",
    "submission_predictions = mlp_lda.predict(lda.transform(x_submission))\n",
    "\n",
    "df_submission['Survived'] = [int(i) for i in submission_predictions]\n",
    "cols_to_write = ['PassengerId', 'Survived']\n",
    "\n",
    "df_submission[cols_to_write].to_csv('kaggle_titanic_submission_jun182018_918pm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3684210526315789"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just curious what % of submission is predicted to survive.\n",
    "# If it is like train.csv, then it should be around 40%, I think.\n",
    "df_submission[df_submission.Survived > .4].shape[0] / df_submission.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Kaggle, the accuracy score that places the model at 8846th place out of 11403 was: 0.76076. This, of course, is far below what was expected. Kaggle's accuracy score is only on a portion of the submission, not the entirety. So, it is possible that it just chose some of the worst predictions. That, however, is probably not the case, so I am not sure why this happened.\n",
    "\n",
    "UPDATE 01; Jun. 17 7:36 PM:\n",
    "Only standard scaling Age and Fare improved Kaggle accuracy to 0.78468, which puts the model in 4624th place.\n",
    "\n",
    "UPDATE 02; Jun. 18 9:18 PM:\n",
    "Transforming the data with linear discriminant analysis and then training a neural network on the LDA transformed data improved accuracy up to 0.79425, which puts this model at 2,532nd place out of 11,356 people. This just goes to show either that maybe something's up with Kaggle or that building generalizable models is difficult (for me, at least). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
